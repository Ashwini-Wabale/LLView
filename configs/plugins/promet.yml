# Copyright (c) 2023 Forschungszentrum Juelich GmbH.
# This file is part of LLview. 
#
# This is an open source software distributed under the GPLv3 license. More information see the LICENSE file at the top level.
#
# Contributions must follow the Contributor License Agreement. More information see the CONTRIBUTING.md file at the top level.
#
# Contributors:
#    Filipe Guimar√£es (Forschungszentrum Juelich GmbH) 

# Each top-level entry correspond to a prometheus server
prometheus:
  hostname: ${PROMETHEUS_SERVER}

  # When no authentication is necessary, use:
  # credentials: "none"
  #
  # Otherwise credentials can be given here (using keys 'username' and 'password')
  # (envvars can be used)
  # credentials:
  #   username: user
  #   password: password
  #
  # Or obtained via a function "get_user_pass" in module "credentials" (whoud be in PYTHONPATH)
  # in this case, use 
  credentials: "module"
  # The following lines are then run:
  #
  #   from credentials import get_user_pass
  #   username,password = get_user_pass()
  #
  # If none of these are given, name and password are 
  # asked directly for the user (if no username is given at this point, the queries are done without authentication)

  # Different (XML) files to be created can be given here
  files:
    cpus:
      LML: './cores_LML.xml'
      type: 'cpuinfo'
      prefix: 'cpu'
      metrics:
        coreidle:
          query: 'sum by (instance, cpu)(irate(node_cpu_seconds_total{job="node-compute"}[5m]))'
          default: 0
          cache: true # Keep query result in memory to use later
      mapping:
        cpu_ts: 'cpu_ts'
        id: 'name'
        usage: 'usage'
        physcoresused: 'physcoresused'
        logiccoresused: 'logiccoresused'

    cores:
      LML: './cores.percore_LML.xml'
      type: 'coreinfo'
      prefix: 'ci'
      metrics:
        coreidle:
          query: 'sum by (instance, cpu)(irate(node_cpu_seconds_total{job="node-compute"}[5m]))'
          default: 0
          cache: true # Keep query result in memory to use later
      mapping:
        ci_ts: 'ci_ts'
        id: 'name'
        percore: 'percore'

    ibms:
      LML: './ibms_LML.xml'
      type: 'fbnode'
      prefix: 'fb'
      metrics:
        fb_mbin:
          query: 'sum by (instance) (irate(node_infiniband_port_data_received_bytes_total{job="node-compute"}[10m]))'
          factor: 0.000000953674316 # 1/1024/1024
          default: 0
        fb_mbout:
          query: 'sum by (instance) (irate(node_infiniband_port_data_transmitted_bytes_total{job="node-compute"}[10m]))'
          factor: 0.000000953674316 # 1/1024/1024
          default: 0
        fb_pckin:
          query: 'sum by (instance) (irate(node_infiniband_port_packets_received_total{job="node-compute"}[10m]))'
          default: 0
        fb_pckout:
          query: 'sum by (instance) (irate(node_infiniband_port_packets_transmitted_total{job="node-compute"}[10m]))'
          default: 0

    gpu:
      LML: './gpus_LML.xml'
      type: 'node'
      prefix: 'gpu'
      metrics:
        # node_up:
        #   query: 'sum by(instance) (up{job="node-compute"})'
        # dcgm_up:
        #   query: 'sum by(instance) (up{job="dcgm"})'
        gpu_util: 
          query: 'sum by(instance,device) (DCGM_FI_DEV_GPU_UTIL)'
          default: 0
        gpu_active:
          query: 'sum by(instance,device) (DCGM_FI_PROF_SM_ACTIVE)'
          default: 0
        gpu_temp:
          query: 'sum by(instance,device) (DCGM_FI_DEV_GPU_TEMP)'
          default: 0
        gpu_sclk:
          query: 'sum by(instance,device) (DCGM_FI_DEV_SM_CLOCK)'
          default: 0
        gpu_pu:
          query: 'sum by(instance,device) (DCGM_FI_DEV_POWER_USAGE)'
          factor: 1000.0
          default: 0
        gpu_clk:
          query: 'sum by(instance,device) (DCGM_FI_DEV_MEM_CLOCK)'
          default: 0
        gpu_memu:
          query: 'sum by(instance,device) (DCGM_FI_DEV_FB_USED)'
          factor: 1048576 # 1*1024*1024
          default: 0
        gpu_memf:
          query: 'sum by(instance,device) (DCGM_FI_DEV_FB_FREE)'
          factor: 1048576 # 1*1024*1024
          default: 0
        gpu_memur:
          query: 'sum by(instance,device) (DCGM_FI_DEV_MEM_COPY_UTIL)'
          default: 0
        gpu_memt:
          default: 41943040 # 40GB*1024*1024
        ncores:
          default: 4992 # Value to be used on LLview client
        gpu_clkr:
          query: 'sum by(instance,device) (DCGM_FI_DEV_CLOCK_THROTTLE_REASONS)'
          default: 0
        pcierx:
          query: 'sum by(instance,device) (DCGM_FI_PROF_PCIE_RX_BYTES)'
          factor: 0.0009765625 # 1/1024.0
          default: 0
        pcietx:
          query: 'sum by(instance,device) (DCGM_FI_PROF_PCIE_TX_BYTES)'
          factor: 0.0009765625 # 1/1024.0
          default: 0
        nvlinkrx:
          query: 'sum by(instance,device) (DCGM_FI_PROF_NVLINK_RX_BYTES)'
          factor: 0.0009765625 # 1/1024.0
          default: 0
        nvlinktx:
          query: 'sum by(instance,device) (DCGM_FI_PROF_NVLINK_TX_BYTES)'
          factor: 0.0009765625 # 1/1024.0
          default: 0

    loadmem:
      LML: './loadmem_LML.xml'
      type: 'loadmemnode'
      prefix: 'lm'
      metrics:
        memavail:
          query: 'sum by (instance) (node_memory_MemAvailable_bytes{job="node-compute"})'
          default: 0
        memfree:
          query: 'sum by (instance) (node_memory_MemFree_bytes{job="node-compute"})'
          default: 0
        memtotal:
          query: 'sum by (instance) (node_memory_MemTotal_bytes{job="node-compute"})'
          default: 0
        cpuload:
          query: 'sum by (instance) (node_load1{job="node-compute"})'
          default: 0
