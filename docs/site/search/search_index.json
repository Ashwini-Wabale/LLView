{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"Welcome to the LLview Documentation <p>LLview is a set of software components to monitor clusters that are controlled by a resource manager and a scheduler system. It collects previously existing data from the system, and presents it to the user via a web portal.  As an example, the resource manager itself can provide information that are available directly from the login nodes; the same applies to performance metrics about the file system that are stored in a central location.  Additional daemons may be used to gather extra information from the compute nodes, keeping the overhead at a minimum, as the metrics are acquired in the range of minutes apart.  With the additional data sources, the LLview portal establishes a link between performance metrics and individual jobs to provide a comprehensive job reporting interface.</p>"},{"location":"access/","title":"Access","text":""},{"location":"access/#access-llview","title":"Access LLview","text":"<p>Here you can access LLview on the different systems of the J\u00fclich Supercomputing Centre:</p> <p>JUWELS Cluster JUWELS Booster JURECA-DC JUSUF JEDI DEEP </p>"},{"location":"contact/","title":"Contact","text":""},{"location":"contact/#contact","title":"Contact","text":"<p>If you have problems, questions, suggestions or complaints about LLview, send us an email to:</p> <p>llview.jsc@fz-juelich.de</p> <p>For issues on the J\u00fclich Supercomputing Centre systems, please contact support via:</p> <p>sc@fz-juelich.de</p>"},{"location":"install/","title":"Home","text":""},{"location":"install/#llview","title":"LLview","text":"<p>The public version of LLview can be downloaded from GitHub. It currently contains a simplified workload including all the necessary scripts and plugins for a basic functioning. This version includes</p> <ul> <li>Core structure to process the configurations, create SQLite databases and process data for presentation on the web portal</li> <li>JuRepTool to create PDF and interactive HTML reports</li> <li>Plugins for:<ul> <li>Slurm</li> <li>Prometheus and other REST-API</li> <li>Flat files (via regex)</li> <li>GitLab repos</li> </ul> </li> </ul> <p>It still does not cover:</p> <ul> <li>Client and live view (the schematic view of the jobs on the nodes)</li> <li>Other plugins</li> <li>Logger (module that collects LLview's access metrics)</li> </ul> <p>These features will be added in future updates.</p>"},{"location":"install/#software-structure","title":"Software Structure","text":"<p>As an HPC monitoring tool, LLview needs to (1) collect and (2) process the data from the system, and then (3) present them to the end-user. These three components are usually done in three separate places:</p> <ul> <li>Collection of data: This is done (partly) on the HPC system to be monitored, where the user running LLview must have permissions to information of all jobs. This part is called Remote on LLview.</li> <li>Processing of data: Data is processed (and extra metrics may be also collected) every minute for all running jobs. To avoid perturbing the nodes of the HPC system, this is usually done in a separate server. This part is called Server on LLview.</li> <li>Presentation of data: The information is presented to the end-user as a web portal. Therefore, a separate Web Server is recommended. The portal is based on the J\u00fclich Reporting Interface JURI.</li> </ul>"},{"location":"install/#installation-instructions","title":"Installation Instructions","text":"<p>The three components mentioned above need to be installed in that order, as each depends on data from the previous one. The installation instructions are then separated for each of the parts:</p> <ol> <li>Remote</li> <li>Server</li> <li>JURI</li> </ol>"},{"location":"install/accountmap/","title":"Account Map","text":""},{"location":"install/accountmap/#how-to-generate-accountmapxml","title":"How to generate <code>accountmap.xml</code>","text":""},{"location":"install/accountmap/#xml-format","title":"XML-format","text":"<p>The file <code>accountmap.xml</code>, that is generated on the <code>webservice</code> step of the <code>dbupdate</code> action, contains the information of the system accounts, their roles and which projects they should have access to. This information is used to generate the folders and <code>.htaccess</code> for the correct setting of permissions.</p> <p>A mockup example of this file is the following: </p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;lml:lgui&gt;\n  &lt;objects&gt;\n    &lt;object id=\"M1\" name=\"username1\" type=\"mentormap\"/&gt;\n    &lt;object id=\"M2\" name=\"username2\" type=\"mentormap\"/&gt;\n    &lt;object id=\"Q1\" name=\"username1_L\" type=\"pipamap\"/&gt;\n    &lt;object id=\"Q2\" name=\"username3_A\" type=\"pipamap\"/&gt;\n    &lt;object id=\"Q3\" name=\"username3_L\" type=\"pipamap\"/&gt;\n    &lt;object id=\"U1\" name=\"username1\" type=\"usermap\"/&gt;\n    &lt;object id=\"U2\" name=\"username3\" type=\"usermap\"/&gt;\n    &lt;object id=\"S1\" name=\"username1\" type=\"supportmap\"/&gt;\n  &lt;/objects&gt;\n  &lt;information&gt;\n    &lt;info oid=\"M1\" type=\"short\"&gt;\n      &lt;data key=\"id\" value=\"username1\"/&gt;\n      &lt;data key=\"projects\" value=\"project1,project2\"/&gt;\n      &lt;data key=\"ts\" value=\"1705592470\"/&gt;\n      &lt;data key=\"wsaccount\" value=\"username1\"/&gt;\n    &lt;info oid=\"M2\" type=\"short\"&gt;\n      &lt;data key=\"id\" value=\"username2\"/&gt;\n      &lt;data key=\"projects\" value=\"project3\"/&gt;\n      &lt;data key=\"ts\" value=\"1705592470\"/&gt;\n      &lt;data key=\"wsaccount\" value=\"username2\"/&gt;\n    &lt;/info&gt;\n    &lt;info oid=\"P1\" type=\"short\"&gt;\n      &lt;data key=\"id\" value=\"username1\"/&gt;\n      &lt;data key=\"projects\" value=\"project1\"/&gt;\n      &lt;data key=\"ts\" value=\"1705592470\"/&gt;\n      &lt;data key=\"wsaccount\" value=\"username1\"/&gt;\n      &lt;data key=\"kind\" value=\"L\"/&gt;\n    &lt;info oid=\"P2\" type=\"short\"&gt;\n      &lt;data key=\"id\" value=\"username3\"/&gt;\n      &lt;data key=\"projects\" value=\"project1\"/&gt;\n      &lt;data key=\"ts\" value=\"1705592470\"/&gt;\n      &lt;data key=\"wsaccount\" value=\"username3\"/&gt;\n      &lt;data key=\"kind\" value=\"A\"/&gt;\n    &lt;info oid=\"P3\" type=\"short\"&gt;\n      &lt;data key=\"id\" value=\"username3\"/&gt;\n      &lt;data key=\"projects\" value=\"project2,project3\"/&gt;\n      &lt;data key=\"ts\" value=\"1705592470\"/&gt;\n      &lt;data key=\"wsaccount\" value=\"username3\"/&gt;\n      &lt;data key=\"kind\" value=\"L\"/&gt;\n    &lt;/info&gt;\n    &lt;info oid=\"U1\" type=\"short\"&gt;\n      &lt;data key=\"id\" value=\"username1\"/&gt;\n      &lt;data key=\"projects\" value=\"project1,project2,project3\"/&gt;\n      &lt;data key=\"ts\" value=\"1705592470\"/&gt;\n      &lt;data key=\"wsaccount\" value=\"username1\"/&gt;\n    &lt;info oid=\"U2\" type=\"short\"&gt;\n      &lt;data key=\"id\" value=\"username3\"/&gt;\n      &lt;data key=\"projects\" value=\"project1,project2,project3\"/&gt;\n      &lt;data key=\"ts\" value=\"1705592470\"/&gt;\n      &lt;data key=\"wsaccount\" value=\"username3\"/&gt;\n    &lt;/info&gt;\n    &lt;info oid=\"S1\" type=\"short\"&gt;\n      &lt;data key=\"id\" value=\"username1\"/&gt;\n      &lt;data key=\"ts\" value=\"1705592470\"/&gt;\n      &lt;data key=\"wsaccount\" value=\"username1\"/&gt;\n    &lt;/info&gt;\n  &lt;/information&gt;\n&lt;/lml:lgui&gt;\n</code></pre><p></p> <p>This example includes:</p> <ul> <li>Two mentors:<ul> <li><code>username1</code> is a mentor of projects <code>project1,project2</code></li> <li><code>username2</code> is a mentor of project <code>project3</code></li> </ul> </li> <li>Two PIs:<ul> <li><code>username1</code> is the leader of project <code>project1</code></li> <li><code>username3</code> is the leader of projects <code>project2,project3</code></li> </ul> </li> <li>One PA:<ul> <li><code>username3</code> is the administrator of project <code>project1</code></li> </ul> </li> <li>One support:<ul> <li><code>username1</code> belongs to the support staff</li> </ul> </li> <li>Two users:<ul> <li><code>username1</code> is a user of projects <code>project1,project2,project3</code></li> <li><code>username3</code> is a user of project <code>project3</code></li> </ul> </li> </ul> <p>Notes:</p> <ul> <li>It is important that the object <code>id</code> in the list of objects to be the same as the <code>oid</code> in the <code>&lt;info&gt;</code> element that defines it below.</li> <li>To avoid duplication of names in the objects, we add the suffixes <code>_L</code> and <code>_A</code> to the usernames for the Principal Investigator (PI), i.e., the leader of a project and Project Administrator (PA), respectively. These letters are also passed as the value of the keys <code>kind</code> inside the respective <code>&lt;info&gt;</code> element.</li> <li>The key <code>wsaccount</code> should contain the user name used to login to the web service.</li> <li>The groups can have overlap, i.e., a user can be support, mentor, PI/PA and user of a project.</li> <li>A user does not need to be defined in all places. In particular, they can be from support and/or mentor, but not be part of any project.</li> <li>The <code>ts</code> key should include the timestamp the data was acquired.</li> <li>Identation is not necessary.</li> </ul>"},{"location":"install/accountmap/#csv-format","title":"CSV-format","text":"<p>To simplify the generation of the <code>accountmap.xml</code> file, we provide the script <code>$LLVIEW_HOME/da/utils/mapping_csv_to_xml.py</code> to be executed with the command: </p><pre><code>python3 da/utils/mapping_csv_to_xml.py --csv accountmap.csv --loglevel DEBUG --xml accountmap.xml \n</code></pre><p></p> <p>This script expects a CSV file (<code>accountmap.csv</code> in the line above) with the following arrangement (the order should be kept):</p> <pre><code># username, project_mentor, project_pa, project_pi, project_user, support\nusername1, \"project1,project2\", \"\", \"project1\", \"project1,project2,project3\", true\nusername2, \"project3\", \"\", \"\", \"\", false\nusername3, \"\", \"project1\", \"project2,project3\", \"project1,project2,project3\", false\n</code></pre> <p>It will then generate a <code>accountmap.xml</code> file with the contents shown above.</p>"},{"location":"install/addmetrics/","title":"Adding New Metrics","text":""},{"location":"install/addmetrics/#adding-new-metrics","title":"Adding new metrics","text":"<p>When a new source of metrics become available, there are a few steps that must be done to start collecting them and make it available on the front-end.</p> <ol> <li>The first step is to create a plugin that will read the metrics from its source and generate an LML file with the desired quantities. One of the existing plugins in the folder <code>${LLVIEW_HOME}/da/rms/</code> (e.g., SLURM, Prometheus or Gitlab) can be used as examples or as a starting point.</li> <li> <p>Add a step to run the plugin on every loop of LLview. If the plugin must be run on the system to be monitored (as, for example, the Slurm plugin that needs access to the <code>scontrol</code> and <code>sacct</code> commands), then this step should be added in the Remote configuration. Otherwise (which is generally the case), it should be added to the <code>dbupdate</code> action of the Server configuration. Once again, the steps to run existing plugins can be used as examples.</p> <p>2.1. If the plugin is not supposed to run on every update, the script <code>${LLVIEW_HOME}/da/utils/exec_every_n_step_or_empty.pl</code> may be useful.</p> <p>2.2. If the collection should be done asynchronously, it can be added as an action instead, and the generated file can be copied with the script <code>${LLVIEW_HOME}/da/utils/cp_if_newer_or_empty.pl</code>.</p> </li> <li> <p>The new generated LML file should be added to the <code>LMLDBupdate</code> and <code>combineLML_all</code> steps.</p> </li> <li>How the file is distributed in the databases (i.e., if the metrics will be put into existing DBs or into new ones) must be then described in the YAML configuration files in the <code>${LLVIEW_CONF}</code> folders. There are many existing DBs and tables that may be used as examples.</li> <li> <p>To add new metrics to the web portal, configurations on <code>${LLVIEW_CONF}/server/LLgenDB/conf_jobreport</code> can be added or modified. Metrics can be added to new or existing tables, graphs, footer, etc.</p> <p>5.1. The configuration of the existing pages are located in <code>${LLVIEW_CONF}/server/LLgenDB/conf_jobreport/views</code></p> <p>5.2. Adding new metrics to the PDF and HTML reports may require extra configurations, as the metrics are put into <code>.dat</code> files that are used by JuRepTool. For example:</p> <ul> <li>the <code>GPU_&lt;jobid&gt;_node.dat</code> containing the GPU metrics are defined in the dataset <code>GPU_node_dat</code> in <code>${LLVIEW_CONF}/server/LLgenDB/conf_jobreport/data_csv_dat/jobreport_datafiles_gpu.yaml</code>; </li> <li>The filenames themselves need to be put into a database, which is defined in the table <code>jobfiles</code> in the configuration file <code>${LLVIEW_CONF}/server/LLgenDB/conf_jobreport/jobreport_databases.yaml</code>; </li> <li>Then the filenames (together with aggregated metrics) are passed to JuRepTool via a (per-job) json file defined in <code>${LLVIEW_CONF}/server/LLgenDB/conf_jobreport/data_json/jobreport_datafiles_json_jureptool.yaml</code>;</li> <li>Finally, when the information is available to JuRepTool for each job, the metrics can be added on the job reports via their configuration in <code>${LLVIEW_CONF}/jureptool/plots.yml</code></li> </ul> </li> </ol>"},{"location":"install/juri_about/","title":"About","text":""},{"location":"install/juri_about/#about-juri","title":"About JURI","text":"<p>The J\u00fclich Reporting Interface (JURI) is a JavaScript-based program that creates a Web Portal to present the data generated by LLview.</p>"},{"location":"install/juri_folder/","title":"Folder Structure","text":""},{"location":"install/juri_folder/#juri-folder-structure","title":"JURI Folder Structure","text":"<p>The generated files are put into the folder defined in <code>$LLVIEW_DATA/$LLVIEW_SYSTEMNAME</code>. The subfolders that used for the default steps are:</p> <ul> <li><code>templates</code>: HandleBars templates to generate pieces of the webpages</li> <li><code>js</code>: JavaScript files that are used to generate the website<ul> <li><code>ext</code>: External JavaScript libraries</li> </ul> </li> <li><code>css</code>: CSS stylesheets used<ul> <li><code>ext</code>: External CSS libraries</li> </ul> </li> <li><code>utils</code>: Scripts that can be used in installation and configuration of LLview</li> <li><code>img</code>: Images used in the Web Portal</li> <li><code>config</code>: Store the base html template used to build the login page</li> <li><code>json</code>: Store JSON files that define context of some tabs</li> <li><code>fonts</code>: Fonts used on the website</li> </ul> <p>The following files are located in the main folder:</p> <ul> <li><code>login.php</code>: Login page that is build using PHP after a user has verified login</li> <li><code>error404.html</code>: Page to be used when an internal page is not found. It may need to be defined in <code>.htaccess</code> file via the <code>ErrorDocument 404 &lt;folder&gt;/error404.html</code> configuration</li> <li><code>index.html</code>: Main page used to build the portal</li> <li><code>LICENSE</code>: GPLv3 License used by JURI</li> <li><code>README.md</code>: Basic JURI information</li> <li><code>CONTRIBUTING.md</code>: Contributing rules for JURI</li> <li><code>CITATION.cff</code>: How to cite JURI</li> </ul>"},{"location":"install/juri_install/","title":"Installation Instructions","text":""},{"location":"install/juri_install/#juri-installation-instructions","title":"JURI Installation Instructions","text":""},{"location":"install/juri_install/#dependencies","title":"Dependencies","text":"<p>As a Web Portal, most of the dependencies of JURI are placed on the browsers - it should work well at least in all the recent versions of the main ones (Firefox, Chrome and Safari).</p> <p>The functioning of LLview, however - in particular, how the permissions are set -, has some requirements on the Web Server:</p> <ul> <li>Apache<ul> <li>Modules<ul> <li>mod_authz_groupfile</li> <li>mod_headers</li> <li>mod_rewrite</li> </ul> </li> <li>Configuration<ul> <li><code>AllowOverride All</code> (to allow <code>.htaccess</code> configurations)</li> </ul> </li> </ul> </li> <li>PHP</li> </ul> <p>JURI already includes several Open Source libraries:</p> <ul> <li>AG-Grid Community</li> <li>Bootstrap</li> <li>d3</li> <li>Font Awesome (v4)</li> <li>Handlebars</li> <li>JQuery</li> <li>Meimaid</li> <li>Plotly.js</li> <li>svg-pan-zoom</li> </ul>"},{"location":"install/juri_install/#configuration","title":"Configuration","text":""},{"location":"install/juri_install/#transfer-of-data-from-llview-server","title":"Transfer of data from LLview Server","text":"<p>The data to be presented in JURI is generated by the LLview Server and must be copied to some folder to be available on the Web Server.</p> <p>If necessary, create the folder where the data generated by the LLview server will be copied into (denoted here by <code>$LLVIEW_WEB_DATA</code>):     </p><pre><code>mkdir $LLVIEW_WEB_DATA\n</code></pre> If you want to use the <code>transferreports</code> step described in the LLview Server section, make sure to have the keys in <code>~/.ssh/authorized_keys</code>. JURI includes the <code>rrsync.pl</code> tool used for copying the data, such that the key can be added as     <pre><code>from=\"&lt;ip of LLview Server&gt;\",command=\"$JURI_HOME/utils/rrsync.pl -wo $LLVIEW_WEB_DATA\",no-agent-forwarding,no-port-forwarding,no-pty,no-user-rc,no-X11-forwarding &lt;complete public part of the ssh-key&gt;\n</code></pre> Connect once to accept the authenticity of the host.<p></p>"},{"location":"install/juri_install/#user-groups","title":"User groups","text":"<p>We use <code>.htgroups</code> and <code>.htaccess</code> files to allow the correct access for the different roles (User, Principal Investigator, Project Admin, Mentor, Admin/Support).</p> <p>It is therefore important to allow configurations via these files. This is done by adding to the Apache configuration the key:     </p><pre><code>AllowOverride All\n</code></pre><p></p> <p>The groups are defined in the file <code>data/sec_files/.htgroups</code> that is generated and copied from the LLview Server. </p> <p>The absolute path of this file (or relative to the <code>DocumentRoot</code>) is written in the generated <code>.htaccess</code>. For this reason, the folder <code>$LLVIEW_WEB_DATA</code> must be known when they are created. It must be configured in the <code>.llview_server_rc</code> file of LLview Server.</p>"},{"location":"install/juri_install/#htaccess","title":"<code>.htaccess</code>","text":"<p>An important configuration used in JURI is provided either from the configuration of server itself (i.e., Apache) or from a specific <code>.htaccess</code> located on the main <code>$LLVIEW_WEB_DATA</code> folder. A template file is given in the repo, with commented out keys to be used for:</p> <ul> <li><code>.htpasswd</code> files</li> <li>LDAP</li> <li>OpenIDConnect (require Apache configuration) Other setups can also be used. </li> </ul> <p>It may be necessary also to adapt the <code>login.php</code> page, which obtains the verified logged user (via <code>$_SERVER['REMOTE_USER']</code> or <code>$_SERVER['PHP_AUTH_USER']</code>) and compares with the mapping generated by the data from the <code>webservice</code> step of the <code>dbupdate</code> action to generate the respective links on the login page.      Note: If the <code>login.php</code> needs to be updated, it may be useful to remove the link created by <code>$JURI_HOME/utils/linkjuri.sh</code> (see below) and copy the file directly.</p> <p>The top-level <code>.htaccess</code> is also important to define the decompression of the generated <code>.gz</code> files. The provided example in <code>$JURI_HOME/.htaccess</code> contains the relevant configuration.</p> <p>Tip</p> <p>The login page for supporters has a <code>Jump to JobID</code> field to quickly access Detailed HTML reports. A simple setup that also allows a quick links to the reports (e.g., <code>llview.fz-juelich.de/&lt;JobID&gt;</code>) is to add an <code>.htaccess</code> file in the parent of <code>$LLVIEW_WEB_DATA</code> (i.e. <code>$LLVIEW_WEB_DATA/../</code>) with the following configuration: </p><pre><code>RedirectMatch \"^/(.*?)/([0-9]+)$\" \"/$1/login.php?jobid=$2\"\n</code></pre><p></p>"},{"location":"install/juri_install/#installation","title":"Installation","text":"<ul> <li>Make sure the dependencies are satisfied</li> <li>Get JURI (J\u00fclich Reporting Interface):     <pre><code>git clone https://github.com/FZJ-JSC/JURI.git\n</code></pre> This is where the <code>$JURI_HOME</code> should be defined below, and the instructions also use this notation.</li> <li>Link JURI with the data folder: the website (HTML/CSS and JavaScript) are provided by JURI, while the data is coming from LLview Server via the <code>transferreports</code> step. These folders must be linked together inside a given parent folder, denoted here by <code>$LLVIEW_WEB_DATA</code> (defined, for example, by <code>~/system</code>), that will be made accessible via the web. </li> </ul> <p>Inside <code>$LLVIEW_WEB_DATA</code> must be placed:     - the <code>data</code> folder that is copied/synchronized from <code>${LLVIEW_DATA}/${LLVIEW_SYSTEMNAME}/tmp/jobreport/data</code> (see how to transfer data from LLview Server),      - and the symbolic links for the JURI pages and scripts. These links can be create using the script <code>linkjuri.sh</code> provided in <code>$JURI_HOME/utils</code>:         </p><pre><code>$JURI_HOME/utils/linkjuri.sh $JURI_HOME $LLVIEW_WEB_DATA\n</code></pre> - The access setup of the portal can be done on the <code>.htaccess</code> file on the main folder <code>$LLVIEW_WEB_DATA</code> (i.e., the parent of <code>data/</code>). An example is given in JURI's repo.      Note: It is not linked with <code>linkjuri.sh</code> as this file may include sensitive data. - The linked folder <code>$LLVIEW_WEB_DATA</code> can then be exposed to external access, e.g.:     <pre><code>ln -s $LLVIEW_WEB_DATA /srv/www/htdocs/system\n</code></pre><p></p>"},{"location":"install/remote_about/","title":"About","text":""},{"location":"install/remote_about/#about-llview-remote","title":"About LLview Remote","text":"<p>LLview Remote consists of collector plugins that run on the system to be monitored (i.e., the remote system). This is required as some commands are only available there. The workflow of LLview Remote is activated via cronjob to collect the metrics every minute, and put them into LML files. No LLview daemon or monitor keeps running on the system while the metrics are not being collected.</p> <p>Currently, only the Slurm plugins is included, and it runs on the remote part (e.g., on a login node of the system to be monitored) to collect the reports from commands such as <code>scontrol</code> and <code>sacct</code>, by a user with permissions to get information of all jobs.</p>"},{"location":"install/remote_folder/","title":"Folder Structure","text":""},{"location":"install/remote_folder/#remote-folder-structure","title":"Remote Folder Structure","text":"<p>The generated files are put into the folder defined in <code>$LLVIEW_DATA/$LLVIEW_SYSTEMNAME</code>. Three subfolders are then used:</p> <ul> <li><code>logs</code>: Location of the log files</li> <li><code>perm</code>: Folder used by LLview to store files indicating a job is running</li> <li><code>tmp</code>: Location of the temporary LML files, that will also be copied to <code>$LLVIEW_SHARED</code></li> </ul>"},{"location":"install/remote_install/","title":"Installation Instructions","text":""},{"location":"install/remote_install/#llview-installation-instructions","title":"LLview Installation Instructions","text":""},{"location":"install/remote_install/#dependencies","title":"Dependencies","text":"<p>The dependencies of LLview Remote are:</p> <ul> <li>Crontab</li> <li>Perl (&gt;5) <ul> <li>Modules (install with <code>cpan &lt;ModuleName&gt;</code>)<ul> <li>Data::Dumper</li> <li>Getopt::Long</li> <li>Time::Local</li> <li>Time::HiRes</li> <li>FindBin</li> <li>Parallel::ForkManager</li> <li>File::Monitor</li> <li>File::Spec</li> <li>warnings::unused</li> <li>Exporter</li> <li>Storable</li> <li>IO::File</li> <li>POSIX</li> <li>YAML::XS</li> <li>DBI</li> <li>DBD::SQLite</li> <li>Config::IniFiles</li> <li>JSON</li> </ul> </li> </ul> </li> <li>Python (&gt;3.9) (For the Slurm plugin)<ul> <li>Modules (install with <code>pip install &lt;ModuleName&gt;</code>)<ul> <li>pyyaml</li> </ul> </li> </ul> </li> </ul>"},{"location":"install/remote_install/#configuration","title":"Configuration","text":""},{"location":"install/remote_install/#llview_remote_rc","title":"<code>.llview_remote_rc</code>","text":"<p>The main configuration file of LLview Remote is <code>.llview_remote_rc</code>, that should be put on the home folder <code>~</code>. This file export environment variables that will be used by the different scripts of LLview. The existing variables are:</p> <ul> <li><code>$LLVIEW_SYSTEMNAME</code>: Defines the system name</li> <li><code>$LLVIEW_HOME</code>: LLview's home folder, where the repo was cloned</li> <li><code>$LLVIEW_DATA</code>: Folder in which the data will be stored</li> <li><code>$LLVIEW_CONF</code>: Folder with the configuration files (example configuration files is given in <code>$LLVIEW_HOME/configs</code>)</li> <li><code>$LLVIEW_SHARED</code>: A shared folder between LLview Remote and LLview Server, where the generated files from Remote will be written and read by the Server (therefore, it must be the same set up in <code>.llview_server_rc</code> in the Server part)</li> <li><code>$LLVIEW_SHUTDOWN</code>: File to be used to stop LLview's workflow (the cronjob runs, but immediately stops)</li> <li><code>$LLVIEW_LOG_DAYS</code>: Number of days to keep the logs</li> </ul> <p>Extra definitions can be also exported or modules loaded in this file (for example, to satisfy the dependencies).</p>"},{"location":"install/remote_install/#installation","title":"Installation","text":"<ul> <li>Make sure the dependencies are satisfied.</li> <li>Get LLview:     <pre><code>git clone https://github.com/FZJ-JSC/llview.git\n</code></pre> This is where the <code>$LLVIEW_HOME</code> should be defined below, and the instructions use this notation.</li> <li>Configure:<ul> <li>Copy the config folder <code>$LLVIEW_HOME/configs</code> outside the repo. (This action is optional, but it is recommended, to work with a configuration folder that is not attached to the Repository.) This folder contains all the configuration files which defines the specific configuration of what is collected and what will be presented to the users. Note: The folder structure should be kept, as some scripts use <code>$LLVIEW_CONF/remote/(...)</code>.</li> <li>Edit <code>.llview_remote_rc</code> (an example is given in <code>$LLVIEW_HOME/configs/remote</code>) and put it in the home folder <code>~/</code>, as this is the basic configuration file and it is the only way to guarantee it is a known folder at this point. The possible options are listed here.</li> <li>Edit the <code>executehostpattern</code> value on the remote workflow given in <code>$LLVIEW_CONF/remote/workflows/LML_da_slurm.conf</code>. This workflow consists basically in running the Slurm plugin to collect the data from the system. As this usually runs by cronjobs in a login node, to avoid it running in all the nodes (which would generate unnecessary work and data races), the variable <code>executehostpattern</code> should be set to the hostname of the computer where it will run.</li> <li>[Optional] Check configuration of Slurm plugin on <code>$LLVIEW_CONF/plugins/slurm.yml</code> (<code>$LLVIEW_CONF</code> is the configuration folder defined in <code>.llview_remote_rc</code>). The default configuration should work out-of-the-box.</li> </ul> </li> <li>Add cronjob to crontab:     <pre><code>crontab $LLVIEW_HOME/da/workflows/remote/crontab/crontab.add\n</code></pre> Check if the cronjob is added correctly:     <pre><code>$ crontab -l\n* * * * * . ~/.llview_remote_rc ; perl \"$LLVIEW_HOME/da/workflows/remote/crontab/remoteAll.pl\"\n</code></pre></li> </ul> <p>The Remote part of LLview runs the collection of data every minute. This means that there's no daemon that keeps running on the system. This automatic workflow can then be stopped either by removing/commenting out the cronjob (editing with <code>crontab -e</code>), or touching the <code>$LLVIEW_SHUTDOWN</code> file defined in <code>.llview_remote_rc</code>. In the latter case, the scripts still run, but they stop immediately.</p>"},{"location":"install/server_about/","title":"About","text":""},{"location":"install/server_about/#about-llview-server","title":"About LLview Server","text":"<p>LLview Server is the module responsible to transfer the collected metrics (obtained by LLview Remote) into SQLite3 databases, process them and send the required files to the Web Server to be presented for the user via the J\u00fclich Reporting Interface JURI.</p> <p>LLview Server works as a daemon that keeps running and monitoring conditions to run actions. Triggers for these actions can be: modifications in files, interval of time, or a fixed second on the minute.</p>"},{"location":"install/server_folder/","title":"Folder Structure","text":""},{"location":"install/server_folder/#server-folder-structure","title":"Server Folder Structure","text":"<p>The generated files are put into the folder defined in <code>$LLVIEW_DATA/$LLVIEW_SYSTEMNAME</code>. The subfolders that used for the default steps are:</p> <ul> <li><code>monitor</code>: Location of the log files for the main monitor script (that is run by the cronjob)</li> <li><code>logs</code>: Location of the log files<ul> <li><code>steps</code>: Folder storing temporary log files for each step of all actions that are created and overwritten during every cycle and also permanent error files that happen during the steps</li> </ul> </li> <li><code>perm</code>: Folder used by LLview to store files indicating a job is running, counters<ul> <li><code>db</code>: Folder where SQLite databases are located</li> <li><code>keys</code>: Where SSH-keys for the transferreports step are stored</li> <li><code>wservice</code>: Store the mapping of the Web Service accounts</li> </ul> </li> <li><code>tmp</code>: Location of the temporary LML files (e.g., the ones copied from <code>$LLVIEW_SHARED</code>)<ul> <li><code>jobreport</code>: Files generated by the <code>jobreport</code> action<ul> <li><code>data</code>: Folder that is copied to the Web Server</li> <li><code>tmp</code>: Contains the list of jobs JuRepTool needs to process and management files</li> </ul> </li> </ul> </li> <li><code>arch</code>: Location of the archived files<ul> <li><code>db</code>: csv files containing DB information</li> <li><code>LMLall</code>: Daily tarballs containing combined information of jobs</li> </ul> </li> <li><code>jureptool</code>: Folder used for temporary JuRepTool files and also location of default <code>shutdown</code> file (that stops only JuRepTool)<ul> <li><code>results</code>: Where reports are first generated, and then moved to their final location (to avoid problems during copy to the Web Server)</li> </ul> </li> </ul>"},{"location":"install/server_install/","title":"Installation Instructions","text":""},{"location":"install/server_install/#server-installation-instructions","title":"Server Installation Instructions","text":""},{"location":"install/server_install/#dependencies","title":"Dependencies","text":"<p>The dependencies of LLview Server are:</p> <ul> <li>Crontab</li> <li> <p>Perl (&gt;5) </p> <ul> <li>Modules (install with <code>cpan &lt;ModuleName&gt;</code>)<ul> <li>Data::Dumper</li> <li>Getopt::Long</li> <li>Time::Local</li> <li>Time::HiRes</li> <li>Time::Zone</li> <li>FindBin</li> <li>Parallel::ForkManager</li> <li>File::Monitor</li> <li>File::Spec</li> <li>warnings::unused</li> <li>Exporter</li> <li>Storable</li> <li>IO::File</li> <li>POSIX</li> <li>YAML::XS</li> <li>DBI, RPC::PlClient (if OracleDB needs to be used)</li> <li>DBD::SQLite</li> <li>Config::IniFiles</li> <li>JSON</li> <li>Compress::Zlib</li> <li>Archive::Tar</li> <li>LWP::Simple</li> <li>LWP::UserAgent</li> <li>LWP::Protocol::https</li> <li>SVG</li> <li>SVG::TT::Graph::Line</li> <li>Tk</li> <li>Tk::NoteBook</li> <li>Tk::Table</li> <li>Cwd</li> </ul> </li> </ul> </li> <li> <p>Python (&gt;3.9) (For JuRepTool and plugins for Prometheus and Gitlab)</p> <ul> <li>Packages (install with <code>pip install &lt;PackageName&gt;</code>)<ul> <li>matplotlib (&gt;3.5.0)</li> <li>numpy</li> <li>pandas</li> <li>pyyaml</li> <li>plotly</li> <li>cmcrameri</li> <li>requests</li> </ul> </li> <li>gzip (if compressed HTML are to be generated with option --gzip, python must have been installed with <code>gzip</code> capacities)</li> </ul> </li> <li>SQLite3</li> <li>Fonts for JuRepTool<ul> <li>sans-serif: Liberation Sans or Arial</li> <li>monospace: Liberation Mono or Courier New</li> </ul> </li> </ul>"},{"location":"install/server_install/#configuration","title":"Configuration","text":""},{"location":"install/server_install/#llview_server_rc","title":"<code>.llview_server_rc</code>","text":"<p>The main configuration file of LLview Server is <code>.llview_server_rc</code>, that should be put on the home folder <code>~</code>. This file export environment variables that will be used by the different scripts of LLview. The existing variables are:</p> <ul> <li><code>$LLVIEW_SYSTEMNAME</code>: Defines the system name.</li> <li><code>$LLVIEW_HOME</code>: LLview's home folder, where the repo was cloned.</li> <li><code>$LLVIEW_DATA</code>: Folder in which the data will be stored. It is also possible to use another hard drive or file system (depending on the amount of metrics, this may be recommended). Either the driver is directly mounted and defined in <code>$LLVIEW_DATA</code>, or a symbolic link is created:     <pre><code>ln -s /externalvolume/ $LLVIEW_DATA\n</code></pre></li> <li><code>$LLVIEW_CONF</code>: Folder with the configuration files (example configuration files is given in <code>$LLVIEW_HOME/configs</code>).</li> <li><code>$LLVIEW_SHARED</code>: A shared folder between LLview Server and LLview Remote, where the generated files from Remote will be written and read by the Server (therefore, it must be the same set up in <code>.llview_remote_rc</code> in the Remote part).</li> <li><code>$LLVIEW_SHUTDOWN</code>: File to be used to stop LLview's workflow (the cronjob runs, but immediately stops).</li> <li><code>$LLVIEW_LOG_DAYS</code>: Number of days to keep the logs.</li> <li><code>$JUREPTOOL_NPROCS</code>: Number of processors used by JuRepTool. As JuRepTool runs in parallel to the main LLview workflow, it is recommended to initially use <code>export JUREPTOOL_NPROCS=0</code> to deactivate JuRepTool and only activate it when the full LLview cycle is already working.</li> <li><code>$LLVIEW_WEB_DATA</code>: Folder on the Web Server (accessible via https) where the <code>data</code> will be copied to.</li> <li><code>$LLVIEW_WEB_IMAGE</code>: Path of image to be used on the login page, relative to DocumentRoot (starting with <code>/</code>) or relative to <code>$LLVIEW_WEB_DATA</code> (default: <code>img/$LLVIEW_SYSTEMNAME.jpg</code>).</li> <li><code>$PYTHON</code>: This variable is used to launch JuRepTool. It is important to set the PYTHON variable to use the version with the dependencies satisfied.</li> <li><code>$LLVIEW_CONF_FILE</code>: This variable is used to export the location of the file <code>.llview_server_rc</code> itself to be monitored for changes.</li> </ul> <p>Extra definitions can be also exported or modules loaded in this file (for example, to satisfy the Dependencies).</p>"},{"location":"install/server_install/#actions","title":"Actions","text":"<p>The collection and processing of data is done via actions (the first workflow level), which can contain many steps (second workflow level) each. It is recommended to activate actions and steps inside actions little by little, and follow the <code>errlog</code> files, to find eventual issues and solve them as they appear.</p> <ul> <li>Edit action file <code>$LLVIEW_CONF/server/workflows/actions.inp</code> to the relevant actions to be used. It is recommended to start with <code>active=0</code> for all actions and activate them one by one.</li> <li>Edit the configuration options for each action (e.g. <code>$LLVIEW_CONF/server/workflows/LML_da_dbupdate.conf</code>), when needed. It is recommended to start all steps with <code>active=\"0\"</code> and activate them little by little. (Note: a step may have dependencies that must be activated before or together with it.)</li> <li>Important reminders:<ul> <li> After making changes on the configurations that afect the databases (i.e., adding or removing tables), they must be updated. To avoid corrupting the databases or losing data, this step must be done manually. To simplify the task of updating the databases according to the new configurations, we provide the script <code>updatedb</code> in <code>$LLVIEW_HOME/scripts</code>, which can be run as:     <pre><code>updatedb         [updates the db with output on screen]\nupdatedb log     [updates the db appending the output to the log file $LLVIEW_DATA/$LLVIEW_SYSTEMNAME/logs/checkDB.`date +%Y.%m.%d`.log]\nupdatedb viewlog [to view the logfile]\n</code></pre> <code>updatedb viewlog</code> can be used to check for errors (as <code>updatedb log</code> appends the output to the same log file, there may be more than one output accumulated in the same file). There is no problem running this command when the change in the configuration does not affect the databases, so it is recommended to run it after changes in the YAML files.</li> <li> To check the log and error files, we also provide a script <code>taillog</code> in <code>$LLVIEW_HOME/scripts</code> that can be used to simplify following these files with a <code>tail -f</code> command. It can be used by running     <pre><code>taillog [monitor | (actionname)]\n</code></pre></li> <li> Another script provided in <code>$LLVIEW_HOME/scripts</code> that can be used to list all error files in the folders is <code>listerrors</code>.</li> </ul> </li> </ul>"},{"location":"install/server_install/#dbupdate-action","title":"<code>dbupdate</code> action","text":"<p>The <code>dbupdate</code> action mainly performs the collection of metrics into SQLite databases (plus other tasks such as archiving). The configuration of the different databases and their columns, types and values are done via the YAML files inside the <code>$LLVIEW_CONF/server/LLgenDB</code> folder.</p>"},{"location":"install/server_install/#webservice-step","title":"<code>webservice</code> step","text":"<p>This is an important step that should be edited in each installation.</p> <p>To be able to set up the correct permissions for the role-based access of LLview (where users will have access only to their own jobs and projects, while mentors can see jobs on all mentored projects and support can see all jobs), information on the user accounts and projects are needed. This is obtained in the <code>webservice</code> step of the <code>dbupdate</code> action. In the provided example configuration, this information is generated by <code>$LLVIEW_HOME/da/rms/JSCinternal/get_webservice_accounts.pl</code> (not-included) that is run at every 15th update via the script <code>$LLVIEW_HOME/da/utils/exec_every_n_step_or_empty.pl</code> (included), to avoid too many connections to the database. The output of this step should be put in the file <code>$LLVIEW_DATA/$LLVIEW_SYSTEMNAME/perm/wservice/accountmap.xml</code> that contains information to be added in the database. LLview will then use this information to generate the folders and <code>.htaccess</code> for the correct setting of permissions.</p> <p>If needed, Information about the users with support access can be obtained also via the <code>supportinput</code> action.</p> <p>More information about how to create this file here.</p>"},{"location":"install/server_install/#lmldbupdate-step","title":"<code>LMLDBupdate</code> step","text":"<p>This is where all generated LMLs are processed and put into the databases, as defined in the configurations.</p> <p>Note: For the SQL commands to work, the databases and tables must exist. They are created according to the configurations using the <code>updatedb</code> script.</p>"},{"location":"install/server_install/#trigger_jobrep-step","title":"<code>trigger_JobRep</code> step","text":"<p>Immediately after the data is inserted into the database, the <code>jobreport</code> action is triggered by this step, such that the generation of the files in that action can be done in parallel to the remaining steps of the current one.</p>"},{"location":"install/server_install/#combinelml_all-step","title":"<code>combineLML_all</code> step","text":"<p>In this step, all generated LMLs are combined into a single one to be archived (and they may also be used for a replay feature). This step is done after the LMLDBupdate step to be done in parallel to the <code>jobreport</code> action.</p>"},{"location":"install/server_install/#jobreport-action","title":"<code>jobreport</code> action","text":"<p>The <code>jobreport</code> action maily creates the data to be presented to the user and copies them to the Web Server. Its configuration is also done via the YAML files inside the <code>$LLVIEW_CONF/server/LLgenDB</code> folder.</p>"},{"location":"install/server_install/#transferreports-step","title":"<code>transferreports</code> step","text":"<p>The <code>transferreports</code> step inside the <code>jobreport</code> action is used to transfer data securely from the LLview Server to the Web Server. To use this step as it is by default, it is necessary to create an ssh-key pair:     </p><pre><code>cd $LLVIEW_DATA/$SYSTEMNAME/perm/\nmkdir keys\ncd keys\nssh-keygen -a 100 -t ed25519 -C 'LLview job report transport from LLview-Server' -f www_llview_system_jobreport\n</code></pre> This must be created without any passphrase. Then, on the Web Server, the public part of the key must be added in <code>~/.ssh/authorized_keys</code> as:     <pre><code>from=\"&lt;ip of LLview server&gt;\",command=\"&lt;path to rrsync&gt;/rrsync.pl -wo &lt;folder where data will be copied into&gt;\",no-agent-forwarding,no-port-forwarding,no-pty,no-user-rc,no-X11-forwarding &lt;complete public part of the ssh-key&gt;\n</code></pre> Note: the line above must be adapted to include the correct IP where the LLview Server part is running, the path to <code>rrsync.pl</code> on the web server (this tool is packed with JURI in the folder <code>$JURI_HOME/utils</code>), and the public part of the key created above. Finally, the command itself must be updated with the correct values for:     <pre><code>-sshkey $permdir/keys/www_llview_system_jobreport\n-login &lt;login on the web server&gt;\n-port &lt;port used&gt;\n-desthost &lt;webserver address&gt;\n</code></pre> Note: An initial login may be needed to accept the authenticity of the host (<code>ssh &lt;login&gt;@&lt;webserver address&gt;</code> and then <code>yes</code> is enough, even if you get \"Permission denied\" afterwards)<p></p>"},{"location":"install/server_install/#liveview","title":"<code>liveview</code>","text":"<p>The <code>liveview</code> action uses the LLview client to create a live view of the jobs running on the system.</p>"},{"location":"install/server_install/#jureptool","title":"JuRepTool","text":"<p>The <code>liveview</code> action uses the LLview client to create a live view of the jobs running on the system.</p>"},{"location":"install/server_install/#icmap-action","title":"<code>icmap</code> action","text":"<p>To color the nodes in the detailed job reports according to their interconnect group, the information of their cell/rack can be given in an <code>icmap</code> file (usually in the <code>`${LLVIEW_DATA}/${LLVIEW_SYSTEMNAME}/perm</code> folder) containing a list of nodes with the following format:     </p><pre><code># nodelist_range[:str]  cell[:int]\nnd[0001-0005,0015-0020]  1\nnd[0006-0015]  2\n(...)\n</code></pre> This information is then converted into an xml file via the <code>$LLVIEW_HOME/da/utils/get_hostnodemap.py</code> script, which is then imported to the database to be used by the reports.<p></p>"},{"location":"install/server_install/#supportinput-action","title":"<code>supportinput</code> action","text":"<p>One of the options to set the users that have \"Support\" access on LLview is via the <code>supportinput</code> action. This action watches a file (default in <code>${LLVIEW_SHARED}/../config/support_input.dat</code>) that contains a simple list of usernames (one per line). When this file is changed, the file is copied to <code>${LLVIEW_DATA}/${LLVIEW_SYSTEMNAME}/perm/wservice</code>. This file is then used in the <code>webservice</code> step of the <code>dbupdate</code> action.</p>"},{"location":"install/server_install/#compress-archive-and-delete-actions","title":"<code>compress</code>, <code>archive</code> and <code>delete</code> actions","text":"<p>The actions <code>compress</code>, <code>archive</code> and <code>delete</code> perform maintenance actions that are created on the previous steps on the folder <code>${LLVIEW_DATA}/${LLVIEW_SYSTEMNAME}/tmp/jobreport/tmp/mngtactions</code>. They are important to keep the <code>${LLVIEW_DATA}/${LLVIEW_SYSTEMNAME}/tmp/jobreport/data</code> folder clean.</p>"},{"location":"install/server_install/#jureptool_1","title":"JuRepTool","text":"<p>JuRepTool is the LLview module that generates the PDF and HTML detailed reports. It runs as an action triggered by the file <code>${LLVIEW_SYSTEMNAME}/tmp/plotlists.dat</code>, which contains the list of jobs that needs to have their report created. The generated reports are automatically copied and made available on the LLview portal. To setup and use JuRepTool:</p> <ul> <li>Update the config files located under <code>$LLVIEW_CONF/jureptool</code>. The configuration of the script and plots are given in 4 YAML files:<ul> <li><code>config.yml</code>: General configuration such as fontsize, page size, colors, etc. (env vars can be used here) Important: Here it is important to check the <code>timezone</code> and <code>hostname</code> options.</li> <li><code>system_info.yml</code>: Information on the system names, queues and sizes (cores, CPU and GPU memory) Important: For the reports to be generated, the system name (as defined for LLview) and its queues should be listed here.</li> <li><code>plots.yml</code>: Configuration of sections and the graphs to be plotted in each of them. Keywords from <code>system_info.yml</code> may be used in <code>ylim</code>.</li> <li><code>logging.yml</code>: Logging information (filename, format, level)</li> </ul> </li> <li>Add required fonts (Liberation Sans or Arial, Liberation Mono or Courier New). Liberation fonts are Open Source and can be downloaded from Liberation Fonts' GitHub. Fonts can be installed with:     <pre><code>mkdir ~/.local/share/fonts/\ncp &lt;fonts_folder&gt;/*.ttf ~/.local/share/fonts/\nfc-cache -f\nrm -fr ~/.cache/matplotlib\n</code></pre></li> <li>Give a non-zero value for the number of processes to be used by JuRepTool via the variable <code>$JUREPTOOL_NPROCS</code> in <code>.llview_server_rc</code>.</li> </ul>"},{"location":"install/server_install/#installation","title":"Installation","text":"<ul> <li>Make sure the dependencies are satisfied</li> <li> <p>Get LLview:     </p><pre><code>git clone https://github.com/FZJ-JSC/llview.git\n</code></pre> This is where the <code>$LLVIEW_HOME</code> should be defined below, and the instructions use this notation.<p></p> </li> <li> <p>Configuration:</p> <ul> <li>[Optional] Copy and update config folder in <code>$LLVIEW_CONF</code> (an example is given in <code>$LLVIEW_HOME/configs</code>) This folder contains all the configuration files which defines the specific configuration of what is collected and what will be presented to the users. Note: The folder structure should be kept, as some scripts use <code>$LLVIEW_CONF/server/(...)</code>.</li> <li>Edit <code>.llview_server_rc</code> (an example is given in <code>$LLVIEW_HOME/configs/server</code>) and put it in the home folder <code>~/</code>, as this is the basic configuration file and it is the only way to guarantee it is a known folder at this point. The possible options are listed here.</li> <li>Edit the <code>$LLVIEW_CONF/LLgenDB</code>: here is the whole configuration of metrics, databases, etc. This can also be adapted later, but changes here may require the <code>updatedb</code> command to be run to update the databases.</li> </ul> </li> <li>Source the main configuration file, to be able to use the variable in the next steps:      <pre><code>. ~/.llview_server_rc\n</code></pre></li> <li>Add cronjob to crontab:     <pre><code>crontab $LLVIEW_HOME/da/workflows/server/crontab/crontab.add\n</code></pre> Check if the cronjob is added correctly:     <pre><code>$ crontab -l\n# start monitor daemon\n* * * * * . ~/.llview_server_rc ; perl \"$LLVIEW_HOME/da/workflows/server/crontab/serverAll.pl\"\n</code></pre></li> </ul> <p>The server part of LLview involves a daemon that starts to run the first time the <code>serverAll.pl</code> script is called. Further calls will check if it is already running, and will restart it in case it is not. This script will then run the different actions, which can be triggered in different ways (as an example, the basic <code>dbupdate</code> action monitors when a signal file was changed to start the process of copying and processing the data, and then touches a signal file to trigger the <code>jobreport</code> action).</p> <p>The monitor daemon of LLview Server can be stopped either by <code>touch $LLVIEW_SHUTDOWN</code> (this file should be defined in <code>.llview_server_rc</code>) or killing the <code>monitor_file.pl</code> process. Note that the cronjob still restarts every minute - if <code>$LLVIEW_SHUTDOWN</code> exists, the process immediately stops without starting the daemon. To remove it altogether, additionally delete/comment out the cronjob (editing with <code>crontab -e</code>).</p>"},{"location":"install/troubleshooting/","title":"Troubleshooting","text":""},{"location":"install/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>Here we list some problems that may occur during the installation and/or usage of LLview.</p>"},{"location":"install/troubleshooting/#stuck-processes","title":"Stuck processes","text":"<p>LLview uses signal files in <code>${LLVIEW_DATA}/${LLVIEW_SYSTEMNAME}/perm</code> to indicate that steps are still running. It also checks the PID of the process is the same that initiate the process, to confirm the process is still running. It may happen that some action fails but the process does not exit correctly, so the PID still exists. In this case, LLview does not restart this action, which is not processed anymore. To solve this issue, the stuck processes should be killed. One way to check is by running <code>ps -ef | grep perl</code> to list all running Perl processes, and checking in that list the processes that are old (except <code>${LLVIEW_HOME}/da/monitor/monitor_file.pl</code>, all processes should not be older than a couple of minutes). The files <code>${LLVIEW_DATA}/${LLVIEW_SYSTEMNAME}/perm/RUNNING_*</code> can also be removed (although they should be deleted automatically).</p>"},{"location":"install/troubleshooting/#errors-in-db","title":"Errors in DB","text":"<p>When modifications on the configuration of the database (that is, in the YAML files that define the database layout in <code>$LLVIEW_CONF/server/LLgenDB</code>) is done, these changes must be applied before LLview can use the new format. If that is not done, errors on DB (e.g. <code>DBD::SQLite::db (...)</code>) will be output. </p> <p>To fix this problem, the <code>updatedb</code> command can be run: </p><pre><code>updatedb         [updates the db with output on screen]\nupdatedb log     [updates the db appending the output to the log file $LLVIEW_DATA/$LLVIEW_SYSTEMNAME/logs/checkDB.`date +%Y.%m.%d`.log]\nupdatedb viewlog [to view the logfile]\n</code></pre><p></p>"},{"location":"jobreport/","title":"List of Jobs","text":""},{"location":"jobreport/#list-of-jobs","title":"List of Jobs","text":"List of active jobs in LLview <p>The job reporting interface of LLview web portal displays a list of the currently running jobs, which is updated every 1-3 minutes. The list of jobs are separated in tabs:</p> <ul> <li><code>Active jobs</code>: Jobs that are still running;</li> <li><code>Jobs &lt; 3 weeks</code>/<code>History</code> (depending on the user's role): Jobs that finished up to 3 weeks before.</li> </ul> <p>Note</p> <p>Jobs that run for less than ~2min may not show up in the Job Reports, as there may be insufficient data.</p> <p>Each job on the list contains: </p> <ul> <li>Basic information (as job ID, username, project, start and estimated end time, etc.)</li> <li>Collected and derived/aggregated metrics for different quantities (some of them color-coded to indicate good or bad values)</li> <li>Number of errors</li> <li>Links to the detailed interactive report and to the PDF download</li> <li>Color-coded score for the jobs</li> </ul> <p>Note</p> <p>The score is based on simple data at the moment, but might get more sophisticated in future iterations. The score for CPU-only jobs takes into account solely the <code>CPU usage</code> metric. For GPU jobs, additionally the <code>GPU average usage</code> metric is used. In that case, the score of the job is calculated mostly using the <code>GPU average usage</code> metric (97%), with an addition of the <code>CPU usage</code> (3%)<sup>1</sup>.</p> <p>Example</p> <ul> <li>A CPU-only job using half of the available cores through the entirety of the job has a score of 0.5</li> <li>A GPU-enabled job using 3 of the 4 installed GPUs, driven by 3 cores, has a score of \\(\\frac{3}{4}\\cdot 0.97 + \\frac{3}{48}\\cdot 0.03 = 0.73\\), assuming all devices are busy throughout the entire job </li> </ul> <p>The job list can be filtered and sorted by different quantities, and the visibility of different column sections can be chosen on the bottom right of the list. Moreover, when a job is selected, graphs for key performance metrics are shown at the bottom of the screen, with average, minimum and maximum time series. Their sizes may be adjusted via the buttons on the gray bar above them. The information about the jobs as well as the interactive and PDF reports are stored for three weeks. Afterwards, the information is archived and not accessible by the user anymore.</p> <ol> <li> <p>The 97:3 relationship is an attempt to also incorporate the hosting cores of GPU-only jobs within the score. It is inspired by the performance distribution within a JUWELS Booster node for double-precision computation. The installed NVIDIA A100 GPUs provide roughly 97% of the performance of a node (19.5 TFLOP/s per GPU, 4 GPUs per node), while the AMD EPYC Rome CPUs contribute with approximately 3% (16 FP64 operations per cycle per core, 3.35 GHz, 24 cores per socket, 2 sockets per node).\u00a0\u21a9</p> </li> </ol>"},{"location":"jobreport/detailed_reports/","title":"Reports","text":""},{"location":"jobreport/detailed_reports/#detailed-reports","title":"Detailed Reports","text":"Example of metrics and graphs in PDF report <p>Each detailed report is organised into the following sections:</p> <ul> <li>Overview Table   A concise snapshot of job metadata, timing, resources, performance summaries, I/O stats, GPU metrics, and final status.</li> <li>Usage Overview Graph   Time-series trends and overall averages for CPU and GPU utilisation.</li> <li>Metric Graphs   Interactive heatmaps of individual metrics over time or across resources. For a complete list of metrics, see the List of metrics. For additional graph examples, visit Examples.</li> <li>Node List   Allocated nodes coloured by interconnect group, with GPU details and error highlights.</li> <li>Timeline   Chronological bars showing job and step durations coloured by state (interactive details on hover and click).</li> <li>System Errors   Infrastructure-level errors detected during the run (this section appears only when system errors occur).</li> </ul> <p>Tip</p> <p>The job reports accept options using the Slurm <code>--comment</code> field. Currently, the option below is available:</p> <ul> <li><code>llview_plot_lines</code>: In the PDF report, use line plots for each node/GPU instead of colorplots (recommended for jobs on fewer than 16 nodes or GPUs).</li> </ul> <p> </p> Example of interactive graphs in detailed reports <p>Info</p> <p>In the web-based report (accessible via the  link):</p> <ul> <li>Hover over data points to see exact values.  </li> <li>Click-and-drag or pan to zoom and shift axes.  </li> <li>Zoom-lock toggle on the info bar (at the bottom) to synchronise axis ranges across sections.  </li> <li>Download graph data as JSON using the button in the top-right corner of each chart.</li> </ul>"},{"location":"jobreport/examples/","title":"Examples","text":""},{"location":"jobreport/examples/#examples","title":"Examples","text":"<p>Here we list examples on some cases where the reports generated by LLview  were used to identify behaviours or to solve issues either on the system, on the program, or on the job configuration.</p>"},{"location":"jobreport/examples/#high-cpu-load-with-low-cpu-usage","title":"High CPU Load with low CPU usage","text":"<p>A job usually tries to use all cores on the HPC system by generating a load that is compatible to the available cores (in case other resources - such as available memory - are not a restriction).  However, if the configuration or the pinning is not done correctly, if can happen that the load generated by the user program is not distributed correctly among the available cores on the compute node.</p> <p>This can be seen in the list of jobs of LLview by checking the columns CPU Load and CPU Usage:</p> <p> </p> Example of jobs with high CPU Load but low CPU usage <p>Although this can happen show up for jobs that are just running for a couple of minutes, it may indicate that something is wrong on jobs that are running for a longer time.</p> <p>A job that was running with a CPU load close to 256 (the number of possible threads on the compute nodes) but only 50% of CPU usage presented the following distribution per core:</p> <p> </p> Example of average core distribution of a job no using all the cores <p>Even though the number of processes/threads included eventual simultaneous multithreading (SMT) usage, the graph indicates that no logical cores were used. Not only that, but even some of the \"physical cores\" were also not being used.</p> <p>On the HTML report, the distribution of core usage per node was indicated in the following graph:</p> <p> </p> Example of core distribution per node of a job not using all the cores <p>As it can also be seen from this colorplot, not only the \"Logical\" cores (the second half of the cores) were not used, neither some of the \"Physical\" (the first half) cores.</p> <p>The problem in this case was a change on the Slurm configuration that was not passing the variable <code>cpus-per-task</code> to the <code>srun</code> commands. After fixing this issue, the CPU usage went up to close to 200%, as expected.</p>"},{"location":"jobreport/examples/#memory-leak","title":"Memory leak","text":"<p>A common case of errors is when the job runs out of memory. This can be identified via the system report error mentioning <code>oom-killer</code>. In this case, LLview detailed reports also display the message <code>(Out-of-memory)</code> on the initial header summary.</p> <p>The cause of the memory overload can be simply a regular increase of a system size, a misconfiguration,  but also it can originate on a memory leak. This means that the program is constantly allocating more and more memory without freeing it up, even though some variables/matrices are not used anymore.</p> <p> </p> Example of possible memory leak <p>The example above is a typical case where the memory usage increases constantly until it reaches the  maximum amount available on the node (in this case, around <code>85 GiB</code>). This may be the correct behaviour of the code, if it needs to keep storing more and more information as it is produced, but it may worth a check if there is something that may be deallocated or even some other algorithm.</p> <p>Note</p> <p>In LLview job reports, the <code>memory usage</code> graphs (both for CPU and GPU) is scaled by default from 0 up to the memory limit of the device. In the interactive graphs, these limits can be changed for the job limits.</p> <p> </p> Example of possible memory leak every third node <p>In this second scenario, the job uses 12 nodes, and only every third node display a monotonic memory increase. In the top graph, which is the node-averaged memory usage per time, the average line may not be too high, but the maximum reaches the limit of the system, and the program crashes. As before, this can be a case of unfreed memory on those particular nodes.</p> <p> </p> Example of possible memory leak in the second part of the run <p>This third case shows an example of a job that consists of two parts. At the end of the first one, some memory is freed, and the memory usage decreases. However, in the second part it increases again, reaching the limit of the system.</p>"},{"location":"jobreport/examples/#unused-gpus","title":"Unused GPUs","text":"<p>The supercomputers usually have a number of GPUs in each node (at JSC, there are 4 GPUs per node).  Their usage may be requested via the SBATCH argument <code>--gres=gpu:X</code> of Slurm,  and then they must be correctly used by the code. Another common scenario is that the code requests a given amount of nodes, but does not use all the GPUs that are available.</p> <p> </p> Example of unused GPUs <p>This can be clearly seen in the GPU Utilization graph shown above, where the code is using  only 1 GPU per node, and therefore taking longer to finish running and wasting  computing time (as the job could have run in 4 times less nodes).</p> <p> </p> Example of unused GPUs <p>Another example of unused GPUs is shown in a smaller job above. In this case, 2 nodes were requested, but only the first two GPU of the first node and last two GPU of  the second node were being used-while 4 GPUs were budy, another 4 were idle. After the error has been identified via its job report on LLview, the user was informed and fixed the issue.</p> <p> </p> Example of all GPUs being used <p>The new reports confirm that the new job use a single node, with all the GPUs being utilized.</p>"},{"location":"jobreport/examples/#high-temperature-gpu-throttling","title":"High Temperature / GPU Throttling","text":"<p>GPUs are expected to work on a given frequency. However, it may happen that due to different reasons, the working frequency is decreased-which is called throttling. The most recurring cause of throttling is when the GPU operates at a high temperature (which by itself can also have different causes).</p> <p> </p> Example of high temperature in a single GPU <p> </p> Example of Clock Throttle due to high temperature in a single GPU <p> </p> Example of lower frequency due to high temperature in a single GPU <p>The graphs above show how this issue can be identified. In this case, a single node job used 4 GPUs, and one of them had much slower frequency (as seen in the <code>StreamMP Clk</code> plot) than the others. The cause of this speed down can be identified in the <code>Clock Throttle Reason</code> graph as being <code>32:SwThermSlDwn</code> (Software Thermal Slowdown). Finally, the temperature of that given GPU was <code>77\u00b0C</code>, which was not large enough to trigger error signals to the administrators,  but after being warned by the user, the referred node was put in maintenance to be fixed.</p> <p> </p> Example of high temperature in a single GPU <p> </p> Example of Clock Throttle due to high temperature in a single GPU <p> </p> Example of lower frequency due to high temperature in a single GPU <p>In this second example, it is more obvious that a single GPU in a very large job is much hotter than the others. This also causes a clock throttle (i.e., a frequency decrease) of that given GPU in the <code>StreamMP Clk</code> due  to <code>32:SwThermSlDwn</code> (Software Thermal Slowdown). The node containing this GPU was drained and repaired.</p>"},{"location":"jobreport/examples/#gpus-used-with-low-occupation","title":"GPUs used with low occupation","text":"<p>When you first inspect GPU performance, you might check <code>GPU Utilization</code>.  This shows the percentage of time\u2014over the last sampling interval\u2014during which any kernel was running on the GPU.  But beware: a single, simple kernel running constantly will report 100 % Utilization, even if it only uses a tiny slice of the GPU's actual compute resources.</p> <p>A more telling metric for real parallelism is <code>GPU Active SM</code>.  This measures the average fraction of time that at least one warp was active on each Streaming Multiprocessor (SM), then averages across all SMs.  (For reference, an NVIDIA A100 has 108 SMs, and an H100 may have up to 132 SMs.)</p> <p> </p> Example: GPU Utilization = 100 %, but SM occupancy is very low <p>In the chart above, the job drives the GPU continuously (so Utilization reads 100 %) yet SM occupancy remains low.  That gap means many SMs sit idle\u2014if you launch more work or larger kernels, you can improve occupancy and make your code run significantly faster.</p>"},{"location":"jobreport/examples/#load-imbalance","title":"Load imbalance","text":"<p>Another example where the LLview job reports may be of use is to verify poor performance of the code. This can have origins on a wrong setup (as shown in Unused GPUs) or system problems (due to e.g., High Temperature / GPU Throttling). However, it can also happen due to load imbalance in the code, that is, when some nodes have to do more work than others.</p> <p> </p> Example of load imbalance on the nodes <p>In the case above, this is reflected on a CPU load that varies between the nodes.</p> <p>Note</p> <p>In LLview job reports, the <code>CPU Load</code> is obtained from Slurm, which at JSC contains the 1-min load average.</p> <p> </p> Example of load imbalance on the nodes (zoom) <p>When zooming a particular region (which can be done in the detailed reports generated by LLview), the pattern becomes more clear. In this case, the origin was a bad choice of distribution scheduling in OpenMP.</p>"},{"location":"jobreport/examples/#cpu-gpu-load-alternation","title":"CPU-GPU load alternation","text":"<p>Besides helping to identify errors or erroneous configurations, the reports may also help to recognize patterns on the jobs.</p> <p> </p> Job work alternating between CPU and GPU <p>The overview of the job above shows how the workload alternates between CPU and GPU, and where the time is mostly spent. This information can be used to focus performance improvements either on the most time-consuming part or on overlapping calculations, if possible.</p>"},{"location":"jobreport/examples/#others","title":"Others","text":"Beautiful day <p>This just looks like a rainbow in a blue sky. If you have a nice graph send to us too!</p> <p>Have a nice day!</p>"},{"location":"jobreport/metric_graphs/","title":"Metric Graphs","text":""},{"location":"jobreport/metric_graphs/#metric-graphs","title":"Metric Graphs","text":"<p>LLview's detailed reports visualise each metric using interactive heatmaps (colour maps) that highlight resource usage patterns. See here the full list of metrics.</p> <p>Info</p> <p>\u2022 X\u2011axis: either job timestamp or core ID \u2022 Y\u2011axis: resource dimension (node, GPU, superchip) \u2022 Colour: metric value at each (X,Y) point</p> <p> </p> Heatmap of a metric across core IDs (X) and node names (Y) <p> </p> Heatmap of a metric over time (X) and node names (Y) <p>Other examples can be seen here.</p> <p>Above and beside each heatmap you'll also find three summary curves:</p> <ul> <li>Per\u2011timestamp (top): min, average and max values across all resources at that time.  </li> <li>Per\u2011resource (right): time\u2011averaged value per node/GPU/superchip, including its min\u2013max range.  </li> <li>Global summary (top\u2011right): overall min, average and max for the entire job.</li> </ul> <p>Info</p> <ul> <li>Interactive features:  <ul> <li>Hover: display exact values and metadata at the pointer.  </li> <li>Zoom &amp; Pan: click-and-drag or axis controls to focus on regions of interest.  </li> <li>Color-scale dropdowns: choose or reverse the colour map; switch between job-level or system-level bounds.  </li> <li>Menubar (hover): copy a direct link to the graph, view its title/description, use zoom/pan buttons, or download the data as JSON.</li> </ul> </li> </ul> <p>Use these heatmaps and interactive tools to pinpoint hotspots, compare resources over time, and deep-dive into performance trends.</p>"},{"location":"jobreport/metrics_list/","title":"List of metrics","text":""},{"location":"jobreport/metrics_list/#list-of-metrics","title":"List of metrics","text":"<p>Here we list the current metrics and detail their meaning.</p> CoresCPUGPUPowerI/O (per File System)Interconnect <ul> <li>Usage Average core usage over the runtime of the job per node (y-axis) and per core (x-axis) of the node. </li> </ul> <p>Warning</p> <p>The abscissa in this graph are the core IDs instead of the timestamp, and it includes both \"Physical\" cores (first half) as well as the \"Logical\" ones (second-half) for multithreaded processors.</p> <ul> <li> <p>CPU Usage 1-min average usage of the CPU across all cores in a node. For multithreaded processors, the value can go up to 200% using physical and logical cores.</p> </li> <li> <p>Physical Cores Used Numbers of \"Physical cores\" with usage above 25% in the last minute. The \"Physical cores\" in the graphs are represented the first half of the node.</p> </li> <li> <p>Logical Cores Used (For multithreaded processors) Numbers of \"Logical cores\" with usage above 25% in the last minute. The \"Logical cores\" in the graphs are represented by the second half of the node.</p> </li> <li> <p>Load Average number of runnable processes (including those waiting for disk I/O) over the past 1 minute, indicating short-term system load and responsiveness (e.g., <code>1</code> means a load of 1 core on average - not a percentage).</p> </li> </ul> <p>Note</p> <p>The Load is provided by Linux in three numbers: 1-, 5- and 15-min average loads. In the job reports, the <code>Node: Load</code> is obtained from Slurm, which at JSC contains the 1-min Load average.</p> <ul> <li>Memory Usage Amount of allocated RAM memory (in GiB) in the node.</li> </ul> <p>Note</p> <p>In the job reports, the <code>Node: Memory Usage</code> graphs (both for CPU and GPU) is scaled by default from 0 up to the memory limit of the partition. A swich between Job and System limits can be found on the interactive reports.</p> <p>Danger</p> <p>Some system processes may use up to a few GiB of memory on the system, so it is better to plan for 10-15GiB less than the maximum amount.</p> <ul> <li> <p>Active SM Average fraction of time at least one warp was active on a multiprocessor, averaged over all multiprocessors.</p> </li> <li> <p>Utilization Percent of time over the past sample period during which one or more kernels was executing on the GPU.</p> </li> </ul> <p>Warning</p> <p>The <code>Utilization</code> graph reflect the usage of at least one kernel on the GPU - it does not contain information of how much occupied it is. For this reason, it is recommended to check the <code>Active SM</code> metric described below.</p> <ul> <li> <p>Memory Usage Amount of memory (in GiB) used on the device by the context.</p> </li> <li> <p>Temperature Current Temperature (in Celsius) on a given GPU. </p> </li> </ul> <p>Warning</p> <p>Note that high temperatures may trigger slow down of the GPU frequency (see examples of High Temperature / GPU Throttling).</p> <ul> <li> <p>Clk Throttle Reason Information about factors that are reducing the frequency of clocks. These are:</p> <pre><code>1. GpuIdle - Nothing is running on the GPU and the clocks are dropping to Idle state.\n2. AppClkSet - GPU clocks are limited by applications clocks setting.\n3. SwPwrCap - SW Power Scaling algorithm is reducing the clocks below requested clocks because the GPU is consuming too much power.\n4. HWSlowDown - HW Slowdown (reducing the core clocks by a factor of 2 or more) is engaged. This is an indicator of:\n                * Temperature being too high\n                * External Power Brake Assertion is triggered (e.g. by the system power supply)\n                * Power draw is too high and Fast Trigger protection is reducing the clocks\n5.  SyncBoost - This GPU has been added to a Sync boost group with nvidia-smi or DCGM in order to maximize performance per watt. All GPUs in the sync boost group will boost to the minimum possible clocks across the entire group. Look at the throttle reasons for other GPUs in the system to see why those GPUs are holding this one at lower clocks.\n6.  SwThermSlDwn - SW Thermal Slowdown. This is an indicator of one or more of the following:\n                   * Current GPU temperature above the GPU Max Operating Temperature\n                   * Current memory temperature above the Memory Max Operating Temperature\n7.  HwThermSlDwn - HW Thermal Slowdown (reducing the core clocks by a factor of 2 or more) is engaged. This is an indicator of:\n                   * Temperature being too high\n8.   PwrBrakeSlDwn - Power brake throttle to avoid that given racks draw more power than the facility can safely provide.\n</code></pre> </li> </ul> <p>Note</p> <p>The <code>Clk Throttle Reason</code> graphs are not shown when no throttling was ever active for the job.</p> <ul> <li> <p>StreamMP Clk Current frequency in MHz of SM (Streaming Multiprocessor) clock. The frequency may be slowed down for the reasons given above.</p> </li> <li> <p>Memory Usage Rate Percent of time over the past sample period during which global (device) memory was being read or written. </p> </li> <li> <p>Memory Clk Current frequency of the memory clock, in MHz.</p> </li> <li> <p>Performance State The current performance state for the GPU. States range from P0 (maximum performance) to P12 (minimum performance).</p> </li> </ul> <p>Note</p> <p>The <code>Performance State</code> graphs are only shown when it differs from the default value of <code>0</code>.</p> <ul> <li> <p>PCIE TX The GPU-centric transmission throughput across the PCIe bus (in GiB/s) over the past 20ms.</p> </li> <li> <p>PCIE RX The GPU-centric receive throughput across the PCIe bus (in GiB/s) over the past 20ms.</p> </li> </ul> <p>Warning</p> <p>The <code>PCIE TX</code> and <code>PCIE RX</code> graphs only include throughput via PCIe bus, i.e., between GPU and CPU.</p> <ul> <li> <p>NVLink TX The rate of data transmitted over NVLink in in GiB/s.</p> </li> <li> <p>NVLink RX The rate of data received over NVLink in GiB/s.</p> </li> </ul> <p>LLview can report power metrics (in Watts) at several levels:</p> <ul> <li>Node Power The total power draw for the entire node at the moment of sampling.</li> </ul> <p>Note</p> <p>\"Node Power\" values come from Slurm's <code>CurrentWatts</code> field (<code>scontrol show nodes</code>) and are snapshots taken once per minute. LLview may integrate these samples over time to estimate total energy consumption.</p> <ul> <li> <p>CPU Power The instantaneous power consumed by the CPU package, including its memory controllers and system I/O.</p> </li> <li> <p>CPU Power Cap The enforced power limit on the CPU package. Displaying this is useful when users can modify CPU power caps or when they deviate from the system default.</p> </li> <li> <p>GPU Power The current power draw of each GPU device, including its onboard memory.</p> </li> <li> <p>Superchip Power On Grace\u2013Hopper systems, LLview also reports the power usage for each \u201csuperchip\u201d (i.e. combined Grace and Hopper modules).</p> </li> </ul> <ul> <li> <p>Read Average read data rate (in MiB/s) in the last minute.</p> </li> <li> <p>Write Average write data rate (in MiB/s) in the last minute.</p> </li> <li> <p>Open/Close Operations Average operation rate (in operations/s) in the last minute.</p> </li> </ul> <ul> <li> <p>Data Input Average data input throughput (in MiB/s) in the last minute.</p> </li> <li> <p>Data Output Average data output throughput (in MiB/s) in the last minute.</p> </li> <li> <p>Packet Input Average package input throughput (in pkt/s) in the last minute.</p> </li> <li> <p>Packet Output Average package output throughput (in pkt/s) in the last minute.</p> </li> </ul> <p>Attention</p> <p>The Interconnect values refer to input and output transfers to/from a given node, so it does not include communications within the node itself. However, I/O data is also included in the transferred data in or out of a node.</p>"},{"location":"jobreport/nodelist/","title":"Node List","text":""},{"location":"jobreport/nodelist/#node-list","title":"Node List","text":"Allocated nodes coloured by interconnect group <p>The Node List displays every allocated node as a labelled box, coloured by its interconnect group\u2014a logical grouping that indicates network proximity (nodes sharing a colour are more tightly connected).</p> <p>When a node encounters an error, its box border turns red to highlight the issue.</p> <p>Inside each node box:</p> <ul> <li>Node name (e.g., <code>node001</code>).</li> <li>GPUs (if present): each GPU is numbered and shows key metrics:  </li> <li>Average GPU Active SM (%) or Utilization (%)  </li> <li>Average memory usage (MiB) or memory usage rate (%)  </li> <li>Average SM clock frequency (MHz) and temperature (\u00b0C)</li> </ul> <p>Use this layout to quickly identify:</p> <ul> <li>Which nodes are in the same interconnect group (for communication locality).  </li> <li>GPU performance variations across nodes.  </li> <li>Outliers or imbalances in resource usage before inspecting detailed graphs.  </li> <li>Which nodes experienced errors (outlined in red).</li> </ul>"},{"location":"jobreport/overview_graph/","title":"Overview Graph","text":""},{"location":"jobreport/overview_graph/#usage-overview-graph","title":"Usage Overview Graph","text":"Time-series trends and overall averages for CPU and GPU usage <p>This graph shows how your job's compute resources were utilised over time and in aggregate:</p> <ul> <li> <p>Time-series plot </p> <ul> <li>CPU (grey line): displays CPU usage (%) when that metric is available; if not, it falls back to CPU load.  </li> <li>GPU (coloured line, GPU jobs only): shows GPU Active SM (%) if supported, otherwise GPU Utilization (%).</li> </ul> </li> <li> <p>Overall averages </p> <ul> <li>Left bar: average CPU usage (or load) over the entire job.  </li> <li>Right bar: average GPU usage across all GPUs (only shown for GPU jobs).</li> </ul> </li> </ul> <p>Use this overview to quickly:</p> <ol> <li>Spot periodic dips or spikes in CPU or GPU activity.  </li> <li>Compare CPU and GPU utilisation at a glance.  </li> <li>Confirm balanced use of available resources throughout the run.</li> </ol>"},{"location":"jobreport/overview_table/","title":"Overview Table","text":""},{"location":"jobreport/overview_table/#overview-table","title":"Overview Table","text":"Job summary and key metrics <p>The Overview Table at the top of each detailed report provides a concise summary of your job:</p> <ul> <li>Job &amp; User: ID, name, user and project.</li> <li>Timing: runtime, submit/start/end times and queue wait.</li> <li>Resources: number of nodes &amp; GPUs with number of timestamp data-point counts.</li> <li>Script: path to the submission script (or indication of interactive jobs).</li> <li>Performance: min/avg/max statistics for CPU, interconnect traffic, packets and node power.</li> <li>I/O: total read/write (MiB) and peak rates per filesystem.</li> <li>GPU: average SM occupancy, memory usage rate, temperature and power.</li> <li>Final Status: job state, return code, core\u2011hours used and estimated energy.</li> </ul> <p>Use this summary to spot issues at a glance before diving into the detailed graphs.</p>"},{"location":"jobreport/system_errors/","title":"System Errors","text":""},{"location":"jobreport/system_errors/#system-errors","title":"System Errors","text":"Example of an Out-of-Memory (OOM) system error <p>At the end of each detailed report, LLview lists System Errors detected during your job run (when that is the case). These include errors emitted from the system side, which users usually do not have access to.</p> <p>Common system errors include:</p> <ul> <li>Out-of-Memory (OOM): Jobs killed by the node's oom-ripper when exceeding available memory.  </li> <li>Node Failures: Hardware or network issues on a node.  </li> <li>Interconnect Link Failures: link failures (see more details about \"Flipping links\" here).</li> </ul> <p>Use this information to:</p> <ul> <li>Diagnose whether failures were due to your code or system instability.  </li> <li>Identify problematic nodes or links that may need attention.  </li> <li>Correlate error timestamps with performance plots to pinpoint root causes.  </li> </ul>"},{"location":"jobreport/timeline/","title":"Timeline","text":""},{"location":"jobreport/timeline/#timeline","title":"Timeline","text":"Job and step timelines coloured by state <p>The timeline shows the full job and each step as horizontal bars coloured by state:</p> <ul> <li>Blue: Running  </li> <li>Green: Completed  </li> <li>Yellow: Timeout  </li> <li>Red: Failed</li> </ul> <p>Info</p> <ul> <li>Static PDF view: provides an immediate overview of when the job and its steps occurred.</li> <li>Interactive HTML:  <ul> <li>Hover for details: job/step name, status, nodes, tasks, CPUs, start/end times, duration, return code, signal.  </li> <li>Click a bar to zoom the X\u2011axis to that step's timespan.  </li> <li>Top\u2011right menubar (on hover) lets you copy a link, use zoom/pan buttons, or download the data as JSON.</li> </ul> </li> </ul> <p>Use this timeline to trace job flow, identify slow or failed steps, and correlate them with performance metrics.</p>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues","title":"Known issues","text":""},{"location":"known_issues/#current-issues","title":"Current issues","text":"<p>Here we list the known issues on LLview.</p>"},{"location":"known_issues/#metrics-on-deep","title":"Metrics on DEEP","text":"<p>Not all metrics are available yet on the DEEP system.  However, as DEEP also works as a test system for new implementations within projects, some metrics can be available only on DEEP and not listed on the documentation.</p>"},{"location":"known_issues/#incorrect-display-in-small-devices","title":"Incorrect display in small devices","text":"<p>LLview was designed to work on desktops, and issues may show up when using small devices such as smartphones. We plan to improve the LLview display on these devices in the future.</p>"},{"location":"known_issues/#fixed-issues","title":"Fixed issues","text":""},{"location":"known_issues/#openclose-operations","title":"Open/Close operations","text":"<p>The number of <code>Open/Close operations</code> on the detailed reports (both PDF and HTML) were a factor 1000000 (one million) too small.  This issue was fixed on 10.02.2023 around 17:00.</p>"},{"location":"live/","title":"Live View","text":""},{"location":"live/#live-view","title":"Live View","text":"LLview Client interface with node map, running jobs and queue <p>The <code>Live</code> tab, as the LLview Client, displays dynamic and interactive view of the system, including:</p> <ul> <li>Stack diagram of jobs ordered by size;</li> <li>A map of the racks and nodes;</li> <li>List of jobs;</li> <li>Queue diagram (with running jobs and predicted ones);</li> </ul> <p>Each of these elements can be hovered and their counter parts in the others will be highlighted.  This view is useful to see how a given job is distributed over the system (and conversely to identify a given job from its distribution), how long it is predicted to run, when the next jobs are predicted to run, and also how is current usage of the system.</p>"},{"location":"live/client/","title":"LLview Client","text":""},{"location":"live/client/#llview-client","title":"LLview Client","text":"LLview Client interface with node map, running jobs and system usage <p>The interactive client is a stand-alone application that can run on any Linux/Windows/MacOS system and shows the current state of the monitored system.  It includes a color-coded list of the running jobs and their distribution along with the racks and nodes, as well as a visual representation of the queue state.  The job list and the system distribution are linked together, and the identification can be seen by hovering the mouse pointer over one of them.  Monitoring data is accessed by HTTPS from LLview web server providing LML files (more details about this data format below).  Currently, the batch mode of the client is used to generate SVG files representing a live view of the system, which is then embedded into the job reporting web portal via the Live tab.</p>"},{"location":"queue/","title":"Queue","text":""},{"location":"queue/#queue","title":"Queue","text":"Queue Details (User Jobs) in User View <p>LLview web portal also displays a view on the current queue list of the system via two pages in the <code>Queue</code> tab.</p>"},{"location":"queue/#queue-overview","title":"Queue Overview","text":"<p>This page contains the queued, running and recently finished jobs for all the users. The metrics displayed in the table includes:</p> <ul> <li>JobID: Slurm Job ID;</li> <li>Owner: Username of the user owner of the job;</li> <li>Project: Budget used for job submission (project ID);</li> <li>Mentor: Mentor of the project;</li> <li>Queue: Partition used for the job;</li> <li>#Nodes: Number of nodes used by the job;</li> <li>State: State of the job (pending, running, failed or completed);</li> <li>Walltime: Requested walltime;</li> <li>Queue Date: Date and time the job entered the queue;</li> <li>Runtime: Overall runtime (for running jobs);</li> <li>Start Date (est): Date and time the job started (or estimate);</li> <li>Est. End Date: Estimated end date of the job (Start Date + requested Walltime);</li> <li>ArrayJobID: The JobID of the first element of the job array (Slurm);</li> <li>ArrayTaskID: Array index of this particular entry, either a single number of an expression identifying the entries represented by this job record (e.g. \"5-1024\") (Slurm);</li> <li>Reservation: Reservation used by the job.</li> </ul>"},{"location":"queue/#queue-details-user-jobs","title":"Queue Details (User Jobs)","text":"<p>The detailed queue only shows jobs for the current user/project (depending on the current view). It includes extra metrics for each job that are not seen by other users/projects. The additional information available are:</p> <ul> <li>Priority: Current priority of the job (the higher, the more probable a job will get scheduled if it fits in the queue);</li> <li>QOS: Quality of service, i.e., if the project still has quota (<code>normal</code>), already used the monthly quota (<code>lowcont</code>) or used the total quota (<code>nocont</code>);</li> </ul> <p>Note</p> <p>At JSC, <code>lowcont</code> and <code>nocont</code> give different priorities to the job. Nevertheless, jobs with those status can still run when there are idle nodes.</p> <ul> <li>Restart: Number of restart cycles during the preparation of the nodes;</li> <li>Position: Current position of the job in the queue;</li> <li>Wait Time: How long the job waited in the queue;</li> <li>Time to Start: Estimated remaining waiting time for the job to start;</li> <li>Reason: Reason the job could not be scheduled yet;</li> <li>Dependency: Slurm job dependencies;</li> <li>ChainID: ID of the job chain, computed from dependency information (for job with dependencies only).</li> </ul> <p>This view also allows a selection of a job in the table to view footer graphs containing the progress of some metrics with time.  The available graphs are:</p> <ul> <li>Position in the Queue</li> <li>Time to Start</li> <li>Priority</li> <li>Reason</li> </ul>"},{"location":"releases/","title":"Internal Releases","text":""},{"location":"releases/#llview-internal-releases","title":"LLview Internal Releases","text":"<p>A simplified package of LLview is also available Open Source on GitHub. See more</p> <p>Developments on the internal releases of LLview are not following the new implementations as the public one. New fixes and improvements done on the internal releases are brought periodically to the public release, but not vice-versa. The versions of the internal \"releases\" are kept in place for simplicity. A single version of JURI is used for both internal and public versions of LLview.</p>"},{"location":"releases/#232-december-16-2024","title":"2.3.2 (December 16, 2024)","text":"Added  <ul> <li>Automatic archiving (in tests, not yet ported to the public release)</li> <li>Possibility to give system status information to be shown on the webportal</li> <li>JuRepTool: Added 'link failure' error recognition</li> <li>JURI: Added possibility for links to status page (and current status) on all headers (file containing status should be updated externally, e.g. via cronjob)</li> <li>JURI: Added possibility to link to user profile (e.g.: JuDoor)</li> <li>JURI: Added possibility to open login page in another window/tab when using middle mouse or ctrl+click on \"home\" button</li> <li>JURI: Add a check if user of 'loginasuser' exists or not</li> <li>JURI: Added 'jump to project' field on login</li> <li>JURI: Added graph for slider (used at the moment for 'Queue Analysis')</li> </ul>  Changed  <ul> <li>Internal improvements</li> <li>JuRepTool: Activated Core metrics by default for JuRepTool reports (must be deactivated if those metrics are not available)</li> <li>JURI: Compressed external js libraries and added js.gz to <code>.htaccess</code></li> <li>JURI: Removed deflate from <code>.htaccess</code></li> <li>JURI: Turned off cache on <code>.htaccess</code> to avoid large memory consumption</li> <li>JURI: Improved style when viewwidth is reduced</li> <li>JURI: Made username in project page clickable</li> </ul>  Fixed  <ul> <li>Fixed filter for admin jobs on <code>plotlists.dat</code> (files were not created, but jobs were being added for JuRepTool)</li> <li>JuRepTool: Fixed 'CPU Usage' in Overview graph</li> <li>JuRepTool: Removed rows containing 'inf' values</li> <li>JURI: Fixed link of 'jump to jobid' field</li> <li>JURI: Fixed buttons when loginasuser is used</li> <li>JURI: Fixed column show/hide</li> </ul>"},{"location":"releases/#224-april-3-2024","title":"2.2.4 (April 3, 2024)","text":"Added  <ul> <li>Added generation of DBgraphs to automatically create dependency graphs (shown as mermaid graphs on the \"Dependency Graphs\" of Support View)</li> <li>JURI: Added CorePattern fonts and style</li> <li>JURI: Added system selector (Support View)</li> <li>JURI: Added buttons on fields in <code>login.php</code></li> <li>JURI: Added home button</li> </ul>  Changed  <ul> <li>JURI: Changed how versions of external libraries are modified (now via links, such that future versions always work with old reports)</li> <li>JURI: Updated plotly.js and removed old one</li> <li>JURI: Changed login.php to use REMOTE_USER (compatible with OIDC too)</li> <li>JURI: Improved favicon SVG</li> </ul>  Fixed  <ul> <li>JuRepTool: Fixed favicon </li> <li>JuRepTool: Fixed timeline zoom sync</li> <li>JuRepTool/JURI: Removed external js libraries versions</li> <li>JURI: Fix graph_footer_plotly.handlebar to have a common root (to avoid xml error)</li> <li>JURI: Fix .pdf.gz extension on .htaccess</li> </ul>"},{"location":"releases/#222-january-16-2024","title":"2.2.2 (January 16, 2024)","text":"Added  <ul> <li>Added new queue on JuRepTool</li> <li>Possibility to use more than one helper function via <code>data_pre</code> (from right to left)</li> <li>Core pattern usage (Support only)</li> <li>Added info button on the top right when a page has a 'description' attribute (JURI)</li> </ul>  Changed  <ul> <li>Changed images on Web Portal to svg</li> <li>Improve footer resize (JURI)</li> <li>Implemented suggestions from Lighthouse for better accessibility (JURI)</li> <li>Colorscales improved, and changed default to RdYlGr (JURI)</li> </ul>"},{"location":"releases/#220-november-13-2023","title":"2.2.0 (November 13, 2023)","text":"Added  <ul> <li>Annotations and gray area to indicate NUMA domains and direct GPU connections</li> <li>Support for Workflow Manager (WFM) (DEEP only)</li> <li>Updated IOI-adapter for v5 (DEEP only)</li> <li>Added JuMonC adapter so users can define custom graphs in reports</li> <li>HetJobs are now shown as workflows, connecting different jobIDs</li> <li>New Projects page (Support View only)</li> <li>Time aggregation automatic tables (replacing <code>_hourly</code> and <code>_daily</code> tables)</li> </ul>  Changed  <ul> <li>New adapter for GPU metrics from Prometheus (not yet active)</li> <li>JuRepTool: Liberation Fonts are now primarily used in reports</li> </ul>  Fixed  <ul> <li>Link to documentation (Help tab) is now shown on all views</li> <li>Moved map jobid-to-day file to folder where all users have access (such filtering suggestions work in all views)</li> </ul>"},{"location":"releases/#210-august-4-2023","title":"2.1.0 (August 4, 2023)","text":"<p>LLview got constant updates in the last months. Apart from the general improvements, an important recent adition was the inclusion of new core usage metrics. New values are added to the tables of active and finished jobs, graphs were added to the footer when a job is selected (including a new footer tab <code>Cores</code>), and new graphs were added to the PDF and HTML reports. </p> <p>The new metrics can help users, admins and support staff to find wrong configuration or pinning on jobs - as discussed in this example.</p> <p>Full Changelog:</p> <ul> <li>Added IOI information (DEEP only)</li> <li>Increased max_entries per table to 5000</li> <li>Fixed wrong 1M factor on Open/Close operations</li> <li>Improved display of \"wait time\" and \"walltime\" to hh:mm</li> <li>Fixed missing steps and stuck \"Running\" state</li> <li>Improved \"Comments\" and \"Job Names\" collection </li> <li>Improved REASON annotations on footer graph of Queued jobs</li> <li>Added column \"Reservation\" on the tables of the Job Reporting portal</li> <li>Added more information when hovering on each step of the timeline on HTML reports</li> <li>Added Queue Date and Wait Time to the PDF and HTML reports</li> <li>Added Slurm script to PDF and HTML reports</li> <li>Added more internal information: LLview usage, Step timings, complexity, timelines, statistics, usage histograms (Support View only)</li> <li>Added new pages and graph pages for System Information: IO, Fabric, Environment, Overview (Support View only)</li> <li>Added Core usage metrics, including new columns and footer graphs on Job Reporting portal, and new graphs on PDF and HTML reports</li> <li>Improved table column names and descriptions</li> <li>Total score use now CPU Usage instead of CPU Load</li> <li>Added Workflow Manager information (DEEP only)</li> <li>Added GPU information from DCDB (DEEP only)</li> <li><code>CANCELLED by &lt;id&gt;</code> is now shown as <code>CANCELLED by &lt;username&gt;</code></li> <li>Internal fixes and improvements</li> </ul>"},{"location":"releases/#200-december-22-2022","title":"2.0.0 (December 22, 2022)","text":"<p>The new version of LLview, released in the end of 2022, has many new features. Besides a major internal restructure - the kernel of LLview was completely re-written - and a redesign of the page, here are some of the new features:</p>  General  <ul> <li>Easier to configure, generalise and add new and aggregated metrics</li> <li>More stable updates, with less missing points</li> <li>Improved values for <code>Interconnect</code> metrics</li> </ul>  Web portal  <ul> <li>Workflow tab, currently containing information about SLURM workflows (chains and arrays)</li> <li>Improved bottom graphs when selecting a job</li> <li>Option to change the colorscale on the job tables</li> <li>Auto-refresh button on the top right of LLview web portal</li> <li>System information and graphs <code>(available for support and admins)</code></li> <li>Internal monitoring of LLview database and timings <code>(available for support and admins)</code></li> <li>Searching for a job ID will search all running and history <code>(available for support and admins)</code></li> </ul>  Detailed reports  <ul> <li>Option to change the colorscale (colors and limits) on interactive reports</li> <li>Timeline on the detailed reports (both interactive and PDF)</li> <li>Click-to-focus Timeline: on the interactive reports, clicking on a step changes the time-interval to the respective range</li> <li>Improved zoom-lock, that can be controled also from the interactive timeline</li> </ul>"},{"location":"releases/public/","title":"Public Releases","text":""},{"location":"releases/public/#llview-public-releases","title":"LLview Public Releases","text":""},{"location":"releases/public/#240-base-june-16-2025","title":"2.4.0-base (June 16, 2025)","text":"<p>New file-parser plugin and extensions to the Prometheus one (more generic for REST-API now). Large rewrite on JuRepTool to generalise plots. Further improvements on JURI, including Queue Analysis and Queue View.</p>  Added  <ul> <li>Slurm plugin: Added query for Slurm accounts from running jobs (still to be added in DB)</li> <li>Added YAML-linter</li> <li>Added plugin to parse files using regex definitions (e.g., for healthchecker logs)</li> <li>Added example configuration for healthchecker (to be used with the file-parser plugin)</li> <li>Added example configuration for job error (<code>joberr.yaml</code>) and node error (<code>nodeerr.yaml</code>) tables</li> <li>Added new envvar variable <code>LLVIEW_DEMO_MODE</code> to activate demo mode</li> <li>JuRepTool: possibility to add red lines to mark graphs (to be used with calibrate)</li> <li>JuRepTool: added \"Download Data\" button for timeline</li> <li>JURI: Added quck buttons to filter RUNNING, COMPLETED and FAILED jobs</li> <li>JURI: Added queue view using bar-plots (including calendar and fixed hoverinfo)</li> <li>JURI: Added jquery-ui for calendar</li> <li>JURI: Added quickfilter to URL to be able to restore it after refresh or link sharing</li> <li>JURI: Added JUPITER logo</li> </ul>  Changed  <ul> <li>Improved and extended documentation</li> <li>Added logo for dark mode in README</li> <li>Improved Apache header files</li> <li>Changed ActiveSM to percentage</li> <li>Extended the 'prometheus' plugin to handle more generic REST-API (possibility to give endpoints, client secret, and more)</li> <li>Generalisations on Slurm plugin (unlimited time in queue, format of Gres, empty responses)</li> <li>Generalisation on <code>get_hostnodename.py</code> to allow multiple expansions</li> <li>Changed <code>remoteAll.pl</code> and <code>serverAll.pl</code> scripts to use same envvars names</li> <li>Increased timeout on Prometheus plugin</li> <li>Improved and optimized workflow on Prometheus plugin (much faster now)</li> <li>Changes from production: improved logging, bug fixes, small improvements</li> <li>JuRepTool: changed the way the overview graph is defined (now configurable)</li> <li>JuRepTool: major rewriting to allow graphs inside the same section get data from different dat files</li> <li>JuRepTool: improved CI tests, that should be now faster and include their own configuration</li> <li>JuRepTool: added energy values on header, when present on the json file</li> <li>JURI: Make background color of colored cells stay in selected rows</li> <li>JURI: Improved mouse resizing of footer (including preventDefault to avoid selecting text when resizing)</li> <li>JURI: Improved CB tables with links</li> <li>JURI: Automatically show column that are used for sorting</li> <li>JURI: Updated plotly.js to 3.0.0</li> <li>JURI: Removed d3.csv parsing for plots</li> <li>JURI: Improved async plots when jobs are quickly selected</li> <li>JURI: Improved slider plots in 'Queue Analysis'</li> <li>JURI: 'username' in project page now clickable (to the user page on that given project)</li> <li>JURI: Generalised parsing of timestamps</li> <li>Other minor improvements in LLview, JuRepTool and JURI</li> </ul>  Fixed  <ul> <li>Fixed <code>.htaccess</code> files to require valid user</li> <li>Fix for \"unparseable\" line in slurm output</li> <li>Fixed escape sequence for Python&gt;=3.13</li> <li>Fixed check of modification date of files, that led to pdf and html reports not being synced.</li> <li>JuRepTool: Fixed zoom-lock for new Plotly version</li> <li>JURI: Fixed adding filter options to page</li> <li>JURI: Unified system names on login page</li> <li>JURI: Added <code>.js.gz</code> to <code>.htaccess</code></li> <li>JURI: Slider bar with steps are updated directly with the event (plotly would not update all traces)</li> <li>JURI: Added 'id' to 'floatingFilters'</li> <li>JURI: Clean up the plots when plotting fails (when selecting a job, the wrong plots could be shown)</li> <li>JURI: Fixed fonts of the core patterns on Chrome</li> <li>Other minor fixes in LLview, JuRepTool and JURI</li> </ul>"},{"location":"releases/public/#232-base-december-16-2024","title":"2.3.2-base (December 16, 2024)","text":"Added  <ul> <li>Improved Gitlab plugin (that runs now by default only every 15min), including possibility to give units of metric</li> <li>Improvements on Prometheus plugin: possibility to authenticate with token, added min/max to metrics, possibility to turn off verification on requests</li> <li>Possibility to give system status information to be shown on the webportal</li> <li>Added pre-set options for grid</li> <li>JuRepTool: Added 'link failure' error recognition</li> <li>JURI: Added possibility for links to status page (and current status) on all headers (file containing status should be updated externally, e.g. via cronjob)</li> <li>JURI: Added possibility to link to user profile (e.g.: JuDoor)</li> <li>JURI: Added parsing of pre-set filter options and button besides the top search (max. filters per column: 3)</li> <li>JURI: Added possibility to open login page in another window/tab when using middle mouse or ctrl+click on \"home\" button</li> <li>JURI: Add a check if user of 'loginasuser' exists or not</li> <li>JURI: Added 'jump to project' field on login</li> <li>JURI: Added helper function to convert number to hhmm</li> <li>JURI: Added graph for slider (used at the moment for coming 'Queue Analysis')</li> </ul>  Changed  <ul> <li>Improved default columns shown on tables (description, conversions, etc)</li> <li>Decrease the default amount of cores used in different steps, to avoid using too much memory</li> <li>Deactivated all but basic Slurm queries by default (and commented out CB in config)</li> <li>Unified 'monitor' logs now also located in the 'logs' folder</li> <li>Improved documentation (including a first version of how to add new metrics)</li> <li>Changed <code>onhover</code> to use list/array instead of dict/object in gitlab plugin (so order is kept)</li> <li>Adapted <code>serverAll</code> search command to be able to use 2 systems in one server</li> <li>Changes from production: internal improvements</li> <li>JuRepTool: Activated Core metrics by default for JuRepTool reports (must be deactivated if those metrics are not available)</li> <li>JURI: Improved grid columns sizing</li> <li>JURI: Automatically open grid column group when a hidden column uses a filter</li> <li>JURI: Compressed external js libraries and added js.gz to <code>.htaccess</code></li> <li>JURI: Removed deflate from <code>.htaccess</code></li> <li>JURI: Turned off cache on <code>.htaccess</code> to avoid large memory consumption</li> <li>JURI: Improved style when viewwidth is reduced</li> <li>JURI: Improved filters (especially for dates)</li> <li>JURI: Made username in project page clickable</li> <li>Other small improvements</li> </ul>  Fixed  <ul> <li>Fixes for absent logic cores (for systems without SMT)</li> <li>Fixed columns when grid is not used (including defaults)</li> <li>Fixed filter for admin jobs on <code>plotlists.dat</code> (files were not created, but jobs were being added for JuRepTool)</li> <li>Create temporary <code>.htgroups_all</code> user to avoid building up support when there's a problem</li> <li>Fixes in <code>monitor_file.pl</code>:  folders not recognized in when given with 2 slashes, folders not created when slash at the end missing</li> <li>JuRepTool: Fixed error output to be also .errlog, to be listed in <code>listerrors</code></li> <li>JuRepTool: Fixed 'CPU Usage' in Overview graph</li> <li>JuRepTool: Removed rows containing 'inf' values</li> <li>JURI: Fixed grid filter size</li> <li>JURI: Fixed link of 'jump to jobid' field</li> <li>JURI: Fixed buttons when loginasuser is used</li> <li>JURI: Fixed column show/hide</li> <li>JURI: Fixed grid filter containing '-' that is not a range</li> <li>Other small fixes</li> </ul>"},{"location":"releases/public/#231-base-july-10-2024","title":"2.3.1-base (July 10, 2024)","text":"<p>Prometheus plugin and GitLab plugin for Continuous Benchmarks! Many fixes and improvements, some of which are listed below.</p>  Added  <ul> <li>Prometheus and Gitlab (for Continuous Benchmark) plugins</li> <li>Brought changes from production version, mainly rsync list of files</li> <li>JuRepTool: Added hash for each graph to URL (also automatically while scrolling)</li> <li>JuRepTool: Added link in plotly graphs to copy the link</li> <li>JURI: Possibility to filter graph data from CSV (with 'where' key)</li> <li>JURI: Added hoverinfo from 'onhover' also on scatter plots</li> <li>JURI: Added possibility to pass trace.line from LLview to plotly graphs</li> </ul>  Changed  <ul> <li>Improved README, with thumbnail</li> <li>Usage-&gt;Utilization for GPU</li> <li>Added ActiveSM in GPU metrics</li> <li>JURI: Changed the storing of headers to save them for each page</li> </ul>  Fixed  <ul> <li>Fixed project link</li> <li>Fixed regex pattern for 'CANCELLED by user' to allow more general usernames</li> <li>Fix for cases where username is in support but not alluser (previously didn't have access to _queued)</li> <li>JuRepTool: Fixed icon sizes in plotly modeBar</li> <li>JuRepTool: Fix for horizontal scroll in nav of html report</li> <li>JuRepTool: adapt for new slurm 'extern' job name</li> <li>JuRepTool: Escape job and step name</li> <li>JuRepTool: Ignore '+0' in step id</li> <li>JuRepTool: Removed deprecated function 'utcfromtimestamp'</li> <li>JuRepTool: Added new tests and fixed old ones (due to new metrics)</li> <li>JuRepTool: Added line break in 'Cancelled by username' in PDF timeline to avoid overlapping text</li> <li>JURI: Fix the size of the footer, to avoid table be under it</li> <li>JURI: Fix the escape of the column group names</li> <li>JURI: Fixes for some gridApi calls that return warnings or errors</li> <li>JURI: Removed autoSizeStrategy and minWidth from column defs, as that breaks their sizing and flexbox</li> <li>JURI: Fixed small bugs when columns or grids are not present</li> </ul>"},{"location":"releases/public/#230-base-may-21-2024","title":"2.3.0-base (May 21, 2024)","text":"<p>Faster tables! Using now ag-grid to virtualise the tables, now many more jobs can be shown on the tables. It also provides a \"Quick Filter\" (or Global Search) that is applied over all columns at once.</p>  Added  <ul> <li>Support for datatables/grids</li> <li>CSV files can be generated </li> <li>New template and Perl script to create grid column definitions</li> <li>Added <code>dc-wai</code> queue on jureptool system config</li> <li>JURI: Added grid (faster tables) support using ag-grid-community library </li> <li>JURI: Helper functions for grid</li> <li>JURI: Grid filters (including custom number filter, which accepts greater '&gt;#', lesser '&lt;#' and InRange '#-#'; and 'clear filter' per column)</li> <li>JURI: Quick filter (on header bar)</li> <li>JURI: Now data can be loaded from csv (usually much smaller than json)</li> </ul>  Changed  <ul> <li>Removed old 'render' field from column definitions (not used)</li> <li>Default Support view now has a single 'Jobs' page with running and history jobs using grid</li> <li>JURI: Adapted to grid:<ul> <li>Buttons on infoline (column groups show/hide, entries, clear filter, download csv)</li> <li>Presentation mode</li> <li>Refresh button</li> <li>Link from jobs on workflows</li> </ul> </li> </ul>  Fixed  <ul> <li>Improved README and Contributing pages</li> <li>Fixed text of Light/Dark mode on documentation page</li> <li>Fixed get_cmap deprecation in new matplotlib version</li> <li>JURI: Small issues on helpers</li> <li>JURI: Fixed color on 'clear filter' link</li> </ul>"},{"location":"releases/public/#224-base-april-3-2024","title":"2.2.4-base (April 3, 2024)","text":"Added  <ul> <li>Added System tab (usage and statistics) for Support View</li> <li>Added option to delete error files on <code>listerrors</code> script</li> <li>Added <code>llview</code> controller in scripts (<code>llview stop</code> and <code>llview start</code> for now)</li> <li>Added power measurements (<code>CurrentWatts</code>) (LML, database and JuRepTool)</li> <li>Added <code>LLVIEW_WEB_DATA</code> option on <code>.llview_server_rc</code> (not hardcoded on yaml anymore, as the envvars are expanded for <code>post_rows</code>)</li> <li>Added <code>LLVIEW_WEB_IMAGE</code> option on <code>.llview_server_rc</code> to change web image file</li> <li>Added <code>wservice</code> and <code>execdir</code> automatic folder creation</li> <li>Added <code>.llview_server_rc</code> to monitor (otherwise, changes in that file required \"hard\" restart)</li> <li>Added <code>icmap</code> action, configuration and documentation</li> <li>Added generation of DBgraphs (from production) to automatically create dependency graphs (shown as mermaid graphs on the \"Dependency Graphs\" of Support View)</li> <li>Added trigger script and step to <code>dbupdate</code> action to use on DBs that need triggering</li> <li>Added options to dump options as JSON or YAML using envvars (<code>LLMONDB_DUMP_CONFIG_TO_JSON</code> and <code>LLMONDB_DUMP_CONFIG_TO_YAML</code>)</li> <li>Added <code>CODE_OF_CONDUCT.md</code></li> <li>JURI: Added CorePattern fonts and style</li> <li>JURI: Added <code>.htpasswd</code> and OIDC examples and general improvements on <code>.htaccess</code></li> <li>JURI: Added system selector when given on setup</li> <li>JURI: Added 'RewriteEngine on' to <code>.htaccess</code> (required for <code>.gz</code> files)</li> <li>JURI: Added buttons on fields in <code>login.php</code></li> <li>JURI: Added home button</li> <li>JURI: Added mermaid graphs and external js</li> <li>JURI: Added svg-pan-zoom to zoom on graphs</li> <li>JURI: Added option to pass image in config</li> <li>JURI: Added \"DEMO\" on system name when new option <code>demo: true</code> is used</li> </ul>  Changed  <ul> <li>Improved <code>systemname</code> in slurm plugin</li> <li>Changed order on <code>.llview_server_rc</code> to match  <code>.llview_remote_rc</code></li> <li>Separated <code>transferreports</code> stat step on <code>dbupdate.conf</code></li> <li>Moved folder creation msg to log instead of errlog</li> <li>Improved documentation about <code>.htaccess</code> and <code>accountmap</code></li> <li>Improved column group names (now possible with special characters and space)</li> <li>Changed name \"adapter\" to \"plugins\"</li> <li>Improved parsing of envvars (that can now be empty strings) from .conf files</li> <li>Further general improvements on texts, logs, error messages and documentation</li> <li>JuRepTool: Improvements on documentation and config files</li> <li>JuRepTool: Moved config folder outside server folder</li> <li>JURI: Adapted <code>login.php</code> to handle also OIDC using REMOTE_USER</li> <li>JURI: Improved favicon</li> <li>JURI: Changed how versions of external libraries are modified (now via links, such that future versions always work with old reports)</li> <li>JURI: Updated plotly.js</li> <li>JURI: Improvements in column group names, with special characters being escaped</li> <li>JURI: Changed footer filename (as it does not include only plotly anymore)</li> </ul>  Fixed  <ul> <li>Fixed <code>starttime=unknown</code></li> <li>Fixed support in <code>.htgroups</code> when there's no PI/PA </li> <li>Fixed <code>'UNLIMITED'</code> time in conversion</li> <li>Fixed creation of folder on SLURM plugin</li> <li>Fixed missing <code>id</code> on <code>&lt;input&gt;</code> element</li> <li>Removed export of <code>.llview_server_rc</code> from scripts (as it resulted in errors when in a different location)</li> <li>JuRepTool: Fixed deprecation messages</li> <li>JURI: Fix <code>graph_footer_plotly.handlebar</code> to have a common root (caused an error in Firefox)</li> <li>JURI: Fix <code>.pdf.gz</code> extension on <code>.htaccess</code> example</li> <li>JURI: Removed some anchors <code>href=#</code> as it was breaking the fragment of the page</li> <li>JURI: Fixed forwarding to job report when using jump to jobID</li> </ul>"},{"location":"releases/public/#223-base-february-13-2024","title":"2.2.3-base (February 13, 2024)","text":"Added  <ul> <li>Added script to convert account mapping from CSV to XML</li> <li>Slurm adapter: Added 'UNKNOWN+MAINTENANCE' state</li> <li>Added link to project in Project tab</li> <li>Added helper scripts in <code>$LLVIEW_HOME/scripts</code> folder and added this folder in PATH</li> <li>JURI: Added CorePattern fonts and style</li> </ul>  Changed  <ul> <li>Added more debug information</li> <li>Further improved installations instructions</li> <li>Slurm adapter: Removed hardcoded way to give system name and added to options in yaml</li> <li>Removed error msg from hhmm_short and hhmmss_short, as they can have values that can't be converted (e.g: wall can also have 'UNLIMITED' argument)</li> <li>JuRepTool: Changed log file extension</li> <li>JURI: Changed how versions of external libraries are modified (now via links, such that future versions always work with old reports)</li> <li>JURI: Removed old plotly library</li> <li>JURI: Changed login.php to use REMOTE_USER (compatible with OIDC too)</li> <li>JURI: Improved favicon SVG</li> </ul>  Fixed  <ul> <li>Fixed wall default</li> <li>Removed jobs from root and admin also from plotlist.dat (to avoid errors on JuRepTool)</li> <li>fixed SQL type for perc_t</li> <li>JuRepTool: Fixed loglevel from command line</li> <li>JuRepTool: Improved parsing of (key,value) pairs</li> <li>JuRepTool: Fixed favicon </li> <li>JuRepTool: Fixed timeline zoom sync</li> <li>JuRepTool/JURI: Removed external js libraries versions</li> <li>JURI: Fix graph_footer_plotly.handlebar to have a common root (to avoid xml error)</li> <li>JURI: Fix .pdf.gz extension on .htaccess</li> </ul>"},{"location":"releases/public/#222-base-january-16-2024","title":"2.2.2-base (January 16, 2024)","text":"Added  <ul> <li>Added link to JURI on README</li> <li>Added troubleshooting page on docs</li> <li>Added description of step <code>webservice</code> on the <code>dbupdate</code> action</li> <li>Added timings in Slurm adapter's LML</li> <li>Added new queue on JuRepTool</li> <li>Possibility to use more than one helper function via <code>data_pre</code> (from right to left)</li> <li>Core pattern example configuration (when information of usage per core is available)</li> <li>Added info button on the top right when a page has a 'description' attribute (JURI)</li> </ul>  Changed  <ul> <li>Changed images on Web Portal to svg</li> <li>Improved installations instructions</li> <li>Lock PR after merge (CLA action)</li> <li>Improved CITATIONS.cff</li> <li>Automatically create shareddir in remote Slurm action</li> <li>Changed name of crontab logs (to avoid problems in case remote and server run on the same place)</li> <li>Improve footer resize (JURI)</li> <li>Implemented suggestions from Lighthouse for better accessibility (JURI)</li> <li>Colorscales improved, and changed default to RdYlGr (JURI)</li> </ul>  Fixed  <ul> <li>Fixed default values of wall, waittime, timetostart, and rc_wallh</li> <li>Improved how logs are cleaned to avoid stuck files</li> <li>Fixed workflow of jobs with a single step</li> </ul>"},{"location":"releases/public/#221-base-november-29-2023","title":"2.2.1-base (November 29, 2023)","text":"Added  <ul> <li>Added Presentation mode (JURI)</li> </ul>  Changed  <ul> <li>Improved the parsing of values from LML to database</li> </ul>  Fixed  <ul> <li>Added missing example configuration files</li> </ul>"},{"location":"releases/public/#220-base-november-13-2023","title":"2.2.0-base (November 13, 2023)","text":"<p>A new package of the new version of LLview was released Open Source on GitHub! Although it does not include all the features of the production version of LLview running internally on the JSC systems, it contains all recent updates of version 2.2.0. On top of that, it was created using a consistent framework collecting all the configurations into few places as possible.</p> <p>The included features are:</p> <ul> <li>Slurm adapter (used to collect metrics from Slurm on the system to be monitored)</li> <li>The main LLview monitor system that collects and processes the metrics into SQLite3 databases</li> <li>JuRepTool, the module to generate HTML and PDF reports</li> <li>Example actions and configurations to perform a full workflow of LLview, including:<ul> <li>collection of metrics</li> <li>processing metrics</li> <li>compressing and archiving</li> <li>transfer of data to Web Server</li> <li>presenting metrics to the users</li> </ul> </li> <li>J\u00fclich Reporting Interface (downloaded separately here), the module to create the portal and present the data to the users</li> </ul> <p>Not included are:</p> <ul> <li>Client (Live view)</li> <li>Other adapters (currently only Slurm)</li> </ul> <p>The documentation page was also updated to include the installation instructions.</p>"},{"location":"workflow/","title":"Workflows","text":""},{"location":"workflow/#workflows","title":"Workflows","text":"List of workflows in LLview <p>LLview now recognizes workflow of jobs (e.g., SLURM arrays and dependency-chains), and show them in the <code>Workflows</code> tab.  The information available per workflow are:</p> <ul> <li>WorkflowID: Unique ID of the workflow;</li> <li>Type: Type of workflow (currently SLURM arrays and chains);</li> <li>Owner: Username of the user owner of the workflow;</li> <li>Project: Budget used for job submission (project ID);</li> <li>Mentor: Mentor of the project;</li> <li>Start Date: Start date and time of first job in workflow;</li> <li>Last Update: Date of last update from scheduler or end time of last job of workflow;</li> <li>#Nodes: Average number of nodes used by the jobs in the workflow;</li> <li>CPU Load: CPU load, 1-min average among all jobs, nodes and over time;</li> <li>JobID List: List of job IDs in the workflow, and their state (<code>P</code>: Pending; <code>R</code>: Running; <code>C</code>: Complete; <code>F</code>: Failed)</li> </ul>"}]}